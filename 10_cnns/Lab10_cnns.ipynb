{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8hMi55H3fX"
      },
      "source": [
        "<img src=\"https://pbs.twimg.com/media/FQK3KkkWUAQsW5L?format=jpg&name=medium\" width = 400>\n",
        "\n",
        "*Image generated by DALL-E 2 upon prompted by \"robot meditating on mathematics, digital art, keywords: synthwave, transcendent, beautiful mind\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfqwZm0OH3fY"
      },
      "source": [
        "# PC Lab 10: Convolutional Neural Networks\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH0OEOmGQ3bm"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY5lpFw0H3fZ"
      },
      "source": [
        "### 1.1 A brief history\n",
        "\n",
        "Convolutional neural networks caused a major step forward in the performance of computer vision. They first became popular because of an annual challenge in image classification: The [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://en.wikipedia.org/wiki/ImageNet#History_of_the_ImageNet_challenge), where models were challenged to classify an image as belonging to one of a thousand(!) different classes. Until 2011, hand-crafted features on images in combination with more traditional machine learning methods such as SVMs reigned supreme. In 2012, a breakthrough was made due to the succesful training of a large-scale CNN. Although the concept of convolutions has been known for quite some decades in machine learning, the technology for large-scale training of CNNs did not take off until this time because it was difficult to train them efficiently (GPU usage was not popular, and no solid platforms for computation on GPUs had been developed). Since then, the field has taken off and multiple developments have come yearly.\n",
        "\n",
        "<img src=\"https://i0.wp.com/semiengineering.com/wp-content/uploads/2019/10/Synopsys_computer-vision-processors-EV7-Fig2-ImageNet.jpeg?ssl=1\" width = 400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycK0JrjlH3fZ"
      },
      "source": [
        "### 1.2 The Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0wOnHRDH3fa"
      },
      "source": [
        "A convolution is the iteration of a kernel with size $ M \\times N $ over a given input $ \\textbf{X} $, performing a 2D linear combination of the weights $ W $  of the kernel with the overlapping area of the input. For a normal convolution with single striding and no padding, the output $ y_{ij} $ is equal to:\n",
        "\n",
        "$$ y_{ij} = \\sum_{a=0}^{m-1} \\sum_{b=0}^{n-1} W_{ab} x_{(i+a)(j+b)}. $$\n",
        "\n",
        "During a convolution, the kernels slides over the input image to obtain a new image of outputs. The stride of a kernel defines the horizontal and vertical stepsize during iteration. Input data can be padded with multiple layers of a zero-filled border, increasing the output dimensions.\n",
        "\n",
        "It is important to understand that a convolution applies the same operation at every local patch in the input. In this sense, convolutions are useful when you expect the input data to contain regularly appearing **local patterns**.\n",
        "\n",
        "You can find visualizations of common convolutional set-ups [here](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md). Several other examples &  an extended explanation on different types of convolutions can be found [here](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZd0ZIxTH3fc"
      },
      "source": [
        "### 1.3 Channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UULBRmWFH3fd"
      },
      "source": [
        "A convolutional neural network usually processes an image with multiple input channels at every pixel. The number of channels can be seen as the dimensionality of every input **token**. In the case of image inputs, the tokens are pixels, and its input dimensionality is three (every pixel is defined by its red/green/blue values). The kernel, although often depicted as only evaluating one channel, actually takes **the sum of all channels** to obtain an output. If we only apply one kernel, we effectively compress the information in our image (since the RGB value of the pixel now has to be represented by a single number). For this reason, multiple parallel kernels are used that each output their own channel. Because of this, we can **learn higher-dimensional spaces of local regions of pixels**. The following image shows the difference between a convolution with one output channel and a convolution with 8 output channels. (The pictured convolution employs no padding, hence the width and height of the image are also affected)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/tfmortie/mlmust/main/10_cnns/channelsdrawing.png\" width = 800>\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>THOUGHT EXERCISE:</b> **How much weights do the above two convolutional layers have? One single bias/intercept number/weight is usually added for every output channel.**\n",
        "\n",
        "</div>\n",
        "\n",
        "The animation below shows an illustrated example with numbers of how a single convolutional kernel (i.e. one output channel) works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFW16Y6xH3fd"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/2560/1*ciDgQEjViWLnCbmX-EeSrA.gif\" width = 800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJf2amkAQ3b7"
      },
      "source": [
        "### 1.4 The CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyh0MsMgQ3b8"
      },
      "source": [
        "A convolutional neural network consists of more than convolutions. Just like with MLPs, we usually add non-linearities such as the ReLU after a linear layer.\n",
        "Another popular operation in convolutional neural networks is a pooling operation such as **max pooling**. Max pooling is a technique that diminishes the dimensionality of the input, resulting in the compression of the image. This compression, in turn, leads to a reduction in the number of parameters within a neural network, and hence, mitigates the risk of overfitting. Compared to convolutional layers, max pooling layers are also initialized with specific arguments such as kernel size, stride and padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGSMvqyRH3ff"
      },
      "source": [
        "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" width = 400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tM2WUMFQ3b9"
      },
      "source": [
        "After many convolutions, we are left with a 3-dimensional object for every image with dimensions $channels \\times width \\times height$. Note that the width and height does not correspond to the original width and height of the input images. The outputs can be considered as \"pseudo\"-pixels, since every remaining \"pseudo\"-pixel has aggregated information from a region of original input pixels. The channels together then represent the vector of information gathered in that \"pseudo\"-pixel.\n",
        "\n",
        "If we want to do classification (or regression) with this \"pseudo\"-image, we need to reshape or flatten the 3-dimensional object to a 1-dimensional vector representation representing the output classes. Hence, a CNN usually consists of a convolutional part, where we extract higher dimensional features of local regions, and an MLP part, where we put linear layers on top of the flattened representations of the image (i.e. letting the information between all regions recombine to get to a final class prediction).\n",
        "\n",
        "Note that this flattening/reshaping is only necessary for classification purposes. If we were doing, for example, image segmentation (classifying every pixel as belonging to a category, not the whole image at once), we would keep our 3-dimensional representation of images.\n",
        "\n",
        "Below is an illustration of a toy example of a cat detector that shows how you can visualize what \"pseudo\"-pixels may signify in a learned neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT8a8M8hQ3b-"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/tfmortie/mlmust/main/10_cnns/catdetectordrawing.png\" width = 800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D1xSKVrQ3b_"
      },
      "source": [
        "Another example showing multi-class classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAP8Iw7H3fe"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg\" width = 600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvJMCnrmH3ff"
      },
      "source": [
        "One can interpret the convolutional layers as the section of the network in which local patterns are extracted (edges, contours, contrasts,...). These are used as inputs for the fully connected neural netwerk, which combines these features to train the classifier.\n",
        "\n",
        "The following picture shows a visualization of what a CNN extracts at each layer, starting from the first layers on the left going deeper towards the right. This visualization is obtained by optimizing an input image to maximally activate the convolution filters.\n",
        "\n",
        "<img src='https://1.bp.blogspot.com/-icbxyuiDoA0/WgEivsyFIgI/AAAAAAAACKo/jsfMgFlfiVA233zXg8xAH3ZAKOchgLb-wCLcBGAs/s1600/image4.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i81s0pxQ3cB"
      },
      "source": [
        "**Final note:** It is important to realize that we have been talking about convolutions on images up until now, these are what we call 2-D convolutions (because they convolve over both width and height). The same concept is also applicable on 1-D sequences, where every input channel can represent information of every input token in that sequence (e.g. a DNA or protein sequence with as tokens bases or AAs and channels as one-hot encodings for which base or AA is present at that position). A 1D Conv would then aggregate patterns from its neighboring inputs (DNA bases or AAs).\n",
        "\n",
        "Even 3-D convolutions are possible on 3-D inputs such as 3-D renders of biologial cells (from e.g. electron microscopy) or 3D MRI-scans. Even a sequence of images (what we know as a video) can be seen as a 3-D object. A 3-D convolution on a video would then extract local patterns per image but also aggregate information from the previous and next frames (in the case of video) OR, in the case of MRI or 3D microscopy: also aggregate information from different depth slices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIT0CU7qQ3cB"
      },
      "source": [
        "### 1.5 Tricks: Residual Connection and Normalization\n",
        "\n",
        "Most recent neural networks use **residual connections** of some sort. A residual connection adds the input of some layer(s) to its output: $y = f(x) + x$. This skip-connection helps with vanishing gradient issues as it essentially lets a part of the input signal skip the layer(s) in $f(.)$. A logical requirement for the use of residual connections is that the output of $f(x)$ has the same dimensionality as the input $x$. For convolutions, this means performing a convolution with padding so that width and height of the image are preserved, and having as much output channels as there were input channels.\n",
        "\n",
        "\n",
        "**Normalization** is another operation that we usually put between layers in order to more easily train our networks. In essence, this ensures that our hidden outputs are numerically well behaved, which speeds up training and helps with better convergence. The most popular ones are batch normalization and layer normalization. More info here: [link](https://www.pinecone.io/learn/batch-layer-normalization/)\n",
        "\n",
        "It is very popular to define a \"Residual Block\" as $z = f(x)+x$, with $f(.) = $ `[Layer -> Activation -> Dropout -> Normalization]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RIWH-PeQ3cC"
      },
      "source": [
        "# 2. Coding\n",
        "\n",
        "In the previous practical, we familiarized ourselves with PyTorch by means of implementing neural network classifiers on the MNIST dataset. In this practical, we will repeat the same workflow, albeit, with a focus on convolutional neural networks instead of neural networks.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07N60wMXQ3cD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZriLRXuXQ3cE"
      },
      "source": [
        "### 2.1 The convolution\n",
        "\n",
        "We'll start by exploring convolutional layers in PyTorch. Note that convolutional layers expect the inputs to be of shape $batchsize \\times channels \\times width \\times height$. The following code shows an example of an input batch, consisting of 8 RGB images (i.e. three channels), each with 28 x 28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgNLbaL0Q3cF"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8, 3, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg_SFi76Q3cG"
      },
      "source": [
        "PyTorch implements 2D convolutions via the [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) class. Check out the documentation to see which options you can specify. Remember to consult the following [page](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md), in order to understand the different arguments. Let's illustrate the usage of some prominent ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJpDwklbQ3cG"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(3, 16, 5) # from 3 to 16 channels with kernel size 5\n",
        "print(x.shape)\n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCi3I0D8Q3cH"
      },
      "outputs": [],
      "source": [
        "# padding = \"same\" will perform padding so as to conserve the input width and height\n",
        "print(x.shape)\n",
        "conv = nn.Conv2d(3, 16, 5, padding = \"same\")\n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wfT91i5Q3cI"
      },
      "outputs": [],
      "source": [
        "print(x.shape)\n",
        "# performing a conv of 2 x 2 every block of 2 by 2 pixels:\n",
        "conv = nn.Conv2d(3, 16, 2, stride = 2)\n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVN_QI_NQ3cI"
      },
      "outputs": [],
      "source": [
        "print(x.shape)\n",
        "conv = nn.Conv2d(3, 8, 3, dilation = 2)\n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CovDyb8RQ3cK"
      },
      "source": [
        "Note that [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) has very similar arguments.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 2.1:</b> **Let's implement our very own custom PyTorch layer: a convolutional residual block. You will be able to use this layer in further exercises when implementing a complete CNN. Refer to the introduction part on residual connections above to see how a residual connection should be constructed. Remember to use appropriate kernel sizes and padding.**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59YArYFvQ3cK"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim = 64, kernel_size = 5):\n",
        "        super().__init__()\n",
        "\n",
        "        # YOUR CODE HERE ....\n",
        "\n",
        "        # TIP: You may want to to use these objects: ..\n",
        "        #nn.Conv2d(...)\n",
        "        #nn.ReLU()\n",
        "        #nn.Dropout()\n",
        "        #nn.BatchNorm2d(...)\n",
        "\n",
        "        # YOUR CODE HERE ....\n",
        "\n",
        "    def forward(self, x):\n",
        "        return None # CHANGE THIS TO YOUR OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFIQCelaQ3cL"
      },
      "outputs": [],
      "source": [
        "ResidualBlock()(torch.randn(2, 64, 16, 16)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-TCkhBjQ3cL"
      },
      "source": [
        "### 2.2 A CNN for MNIST classification\n",
        "\n",
        "With our residual block and PyTorch knowledge from last practical at hand, we can implement our very own convolutional neural network. Let's first recap the most basic building blocks of training PyTorch models:\n",
        "\n",
        "The most basic blueprint of PyTorch model training consists of\n",
        "- Get your data\n",
        "- Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "- Instantiate the model\n",
        "- Instantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "- Iterate through your training data, for every batch:\n",
        "    - reset the gradients\n",
        "    - do forward pass\n",
        "    - compute loss\n",
        "    - backward pass\n",
        "    - update parameters\n",
        "\n",
        "(Optionally):\n",
        "- After every full iteration through all training data samples (called an epoch), loop through all batches of validation data:\n",
        "    - forward pass\n",
        "    - compute loss and validation scores\n",
        "\n",
        "Since we have implemented every necessary step of this process previous practical, for the time being we will focus on developing a proper CNN architecture. In order to test if our model works, we will need to design a model architecture that works for our input image sizes. For this purpose, we can generate some random data in the same size as the MNIST data. Remember that MNIST consists of grayscale images, so they only have one input channel, as opposed to RGB images. They also have a resolution of 28 x 28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnNW-2LfQ3cM"
      },
      "outputs": [],
      "source": [
        "# some generated data in the same shape that we expect our MNIST data:\n",
        "x = torch.randn(8, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtvzWxL9Q3cM"
      },
      "source": [
        "With this generated toy data ready, we can implement a CNN and test it out. We recommend to model the CNN and classifier (MLP) part separately, so we can check the size of the image after it went through the CNN. This size determines how many input nodes the subsequent MLP will require.\n",
        "\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 2.2:</b> **Implement a CNN backbone with at least 3 convolutional operations. You can optionally make use of the residual blocks for the CNN operations. We recommend to also make use of max pooling layers to reduce dimensionality.\n",
        "Because the residual block strictly has the same output and input dimensionality, you should start your network with a normal convolutional layer that returns more than one channel. You can also put convolutions (and max pooling operations) between residual blocks to change the number of hidden dimensions.**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV2-3V9RQ3cN"
      },
      "outputs": [],
      "source": [
        "class CNNBackBone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        return None # CHANGE THIS TO YOUR OUTPUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjFRHkw1Q3cN"
      },
      "source": [
        "Let's see the shape of our fake image batch after a forward pass through the CNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw78HG-uQ3cO"
      },
      "outputs": [],
      "source": [
        "print(x.shape)\n",
        "y = CNNBackBone()(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl3wUcRlQ3cP"
      },
      "source": [
        "As is clear from the output shape above, a flattening operation is required before the output can be passed to the subsequent MLP. In PyTorch, you can use `nn.Flatten`. \n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 2.3:</b> **Combine the CNN backbone with an MLP consisting of 10 output nodes, corresponding to the 10 classes in MNIST. At its simplest, the MLP is a single linear layer that takes in the flattened input and returns 10 ouput nodes. Alternatively, you can make use of the code in the previous practical to quickly implement the MLP.**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFoGwv_6Q3cP"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        return None # CHANGE THIS TO YOUR OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trsljd5WQ3cQ"
      },
      "outputs": [],
      "source": [
        "print(x.shape)\n",
        "y = CNNClassifier()(x)\n",
        "print(y.shape)\n",
        "\n",
        "# a smaller (4 instead of 8) batch of images:\n",
        "x = torch.randn(4, 1, 28, 28)\n",
        "print(x.shape)\n",
        "y = CNNClassifier()(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Fcc7tjQ3cQ"
      },
      "source": [
        "You can use this code to inspect the total number of parameters of your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdBM_7JpQ3cR"
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in CNNClassifier().parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XEWhRyvQ3cR"
      },
      "source": [
        "Keep in mind that a very large model will take a long time to train. For the purpose of this practical, and considering MNIST is quite an \"easy\" dataset, we recommend you to not have more than a hundred thousand parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7kvqekPQ3cR"
      },
      "source": [
        "With our model ready, let's load in the MNIST data as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roXWY5C4Q3cS"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "X_train = train_data.data\n",
        "y_train = train_data.targets\n",
        "\n",
        "X_test = test_data.data\n",
        "y_test = test_data.targets\n",
        "\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "X_train = X_train.unsqueeze(1)\n",
        "X_test = X_test.unsqueeze(1)\n",
        "\n",
        "np.random.seed(42)\n",
        "train_indices, val_indices = np.split(np.random.permutation(len(X_train)), [int(len(X_train)*0.8)])\n",
        "X_val = X_train[val_indices]\n",
        "y_val = y_train[val_indices]\n",
        "X_train = X_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, pin_memory=True, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fj_jgTlQ3cT"
      },
      "outputs": [],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that in comparison with last practical, we do not reshape the data so that every image is respresented by a one-dimensional vector. Instead, we use an `unsqueeze` statement, which adds an extra dimension for the channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PP1UfFOQ3cT"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 2.4:</b> **By means of the training code that was constructed in the previous practical, train your own CNN model. How is it performing in comparison to the model of last practical?**\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "744px",
        "left": "1548px",
        "right": "20px",
        "top": "120px",
        "width": "335px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
