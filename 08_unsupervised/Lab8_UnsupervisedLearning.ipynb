{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Lab 8: Unsupervised Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Unsupervised learning_ is a different branch in machine learning where a response variable $y$ is missing. Unsupervised learning techniques are most often used for exploratory purposes or as a preprocessing step in a supervised context. Unsupervised learning is more prone to subjectivity because results are harder (or even impossible) to validate. This is why one should be careful with the interpretation of results after unsupervised learning. Those interested can have a look at the paper [\"Clustering: Science or Art\"](http://proceedings.mlr.press/v27/luxburg12a/luxburg12a.pdf), which summarizes a couple of critics and tries to give some pointers considering the evaluation of clustering algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this PC-lab we will have a look at two frequently applied techniques in the context of unsupervised learning, namely principal component analysis and k-means clustering. We will end with a general scheme, in which both techniques are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unsupervised](https://analystprep.com/study-notes/wp-content/uploads/2021/03/Img_12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principal components analysis for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gaussianscatterpca](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/800px-GaussianScatterPCA.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular area of unsupervised learning is the area of _Dimensionality Reduction_, in which one tries to reduce the number of variables for visualization purposes or as a preprocessing step for clustering or classification/regression techniques. An established technique which you will find back in most statistics courses is _Principal Components Analysis_ (PCA).\n",
    "\n",
    "Assume a _normalized_ $n\\times p$ data matrix $\\mathbf{X}$. \n",
    "    \n",
    "#### **Goal:** find the direction in $\\mathbf{X}$ with the largest variance (i.e., the most information). \n",
    "\n",
    "In other words, we need to find a linear combination of the inputs:\n",
    "\n",
    "$$ z_{i1} = \\phi_{11}x_{i1}+\\phi_{21}x_{i2}+\\ldots+\\phi_{p1}x_{ip},$$\n",
    "\n",
    ", where $\\mathbf{\\phi}$ is also called the loadings in PCA nomenclature, for which the variance is maximized:\n",
    "\n",
    "$$\\text{maximize}_{\\phi_{11},\\ldots,\\phi_{p1}}\\Big\\{\\frac{1}{n}\\sum_{i=1}^{n}\\Big(\\sum_{j=1}^{p}\\phi_{j1}x_{ij}\\Big)^{2}\\Big\\}\\quad \\text{subject to} \\quad \\sum_{j=1}^{p}\\phi_{j1}^{2}=1.$$\n",
    "\n",
    "Those interested in a non-formal explanation of PCA, check out this intuitive ['dining table-tale'](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) about PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 1.1</b>: \n",
    "**a) Have a look at the [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset. Reduce the dataset using PCA and visualize its first two components using a scatterplot. Don't forget to preprocess your data. Do you see distinctive groups?\n",
    "b) How much variance is captured in the first three components?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: \n",
    "iris = load_iris()\n",
    "X_train = iris.data\n",
    "labels = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##1a): \n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering aims to partition the data in K clusters, so that the within-cluster variation is minimized:\n",
    "\n",
    "$$ \\text{minimize}_{C_{1},\\ldots,C_{K}} \\Big\\{ \\sum_{k=1}^{K}W(C_{k})\\Big\\},$$\n",
    "\n",
    "where the most popular choice for $W(C_{k})$ is the Euclidean distance:\n",
    "\n",
    "$$W(C_{k})=\\frac{1}{|C_{k}|}\\sum_{i,i'\\in C_{k}}\\sum_{j=1}^{p}(x_{ij}-x_{i'j})^{2}.$$\n",
    "\n",
    "K-means clustering uses the following three steps, for which step two and three are repeated until convergence is reached: \n",
    "\n",
    "1) The first step chooses the initial centroids; most easy way of doing this is by choosing K samples at random from the dataset. \n",
    "\n",
    "2) In the second step each element of the dataset is assigned to its nearest centroid. \n",
    "\n",
    "3) New centroids are chosen by taking the mean of all clustered samples according to the previous centroid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 2.1</b>: \n",
    "**Cluster the Iris dataset by means of 2-means and 3-means clustering. Compare the clustering results by visualizing the data in the space induced by the first two principal components.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combining unsupervised techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will find that a number of unsupervised techniques are combined when exploratory analyses are conducted. This is typically the case when your number of variables is high, where you might suffer from the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). In these cases, the approaches laid out above can be combined using the following scheme, which can be tweaked in function of your research question:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Compute the principal components using PCA; \n",
    "\n",
    "2) Select a reduced number of components in function of the explained variance; \n",
    "\n",
    "3) Search for a number of K meaningful clusters; \n",
    "\n",
    "4) Cluster your data using these final settings; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this approach and analyze a more challenging dataset, called the [`digits`-dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html). This dataset consists of handwritten images of the numbers 0-9, which has been proprocessed into feature vectors of length 64. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 3</b>: \n",
    "**Apply the approach illustrated above to the digits dataset. Store and compare the components which explain 50% and 90% of the variance. Choose an 'optimal' number of clusters. What do you think of the result?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def return_noc_pca(var, threshold):\n",
    "    sumvar = 0.\n",
    "    i = 0\n",
    "    while sumvar < threshold: \n",
    "        sumvar+=var[i]\n",
    "        i+=1\n",
    "    return i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
