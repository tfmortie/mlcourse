{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Lab 2: Data Preprocessing and Nearest Neighbors\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nearest neighbor algorithm for classification\n",
    "\n",
    "### Introduction\n",
    "\n",
    "<img src=\"https://www.postnetwork.co/wp-content/uploads/2022/11/irishflowerrs.png\" width=600>\n",
    "\n",
    "### Classification problem\n",
    "\n",
    "In the previous lab session, the iris flower data set was explored. Imagine now the case that for a \n",
    "new iris flower, we know its respective characteristics (sepal and petal length?height, respectively),\n",
    "but do not know its species. A natural task would be to assign the flower to one of the three possible species, based on its characteristics (features). This task (or problem) is called a \n",
    "<strong> classification problem. </strong> In this practical session, an algorithm is considered by looking at the closest training examples in the (labeled) dataset, called nearest neighbour classification.\n",
    "\n",
    "### Dataset\n",
    "In the iris flower dataset (iris120.csv), each instance (i.e. flower) is described by 5 attributes:\\\n",
    "sepal length, sepal width, petal length, petal width and species.\n",
    "For the matter of simplicity, we will only use the species, sepal length and sepal width. \\\n",
    "\\\n",
    "These properties can be seen as _variables_, and for a given flower, each of these variables takes a specific\n",
    "value. In a classification setting, the aim is to predict the value of one of the variables (here the species), based on the value of the other variables (here sepal width and length). The variable of which the values have to be predicted is called the _output variable_ and the variables used to make this prediction are called the _input variables_ or _features_. \\\n",
    "\\\n",
    "A dataset $\\mathcal{D}$ consists of a set of $n$ observations of input-output couples $(\\boldsymbol{x_i}, y_i)$, where $i \\in \\{1, \\dots, n \\}$. Here, $\\boldsymbol{x_i} = (x_{i1}, \\dots, x_{ip}))^T \\in \\mathbb{R}^p$\n",
    "are the observed values for the features (with $p$ the number of features), and $y_i$ are the observed output values, such that a training dataset $\\mathcal{D}_{train}$ with $n$ instances can be written as\n",
    "$$\\mathcal{D}_{train} = \\{ (\\boldsymbol{x_1}, y_1), \\dots, (\\boldsymbol{x_n}, y_n) \\}. $$ \n",
    "\n",
    "### Model and problem setup\n",
    "Using the dataset, the goal is to build a _model_ $f$ that is able to predict the value of the output variable, given the value of the input variables. \\\n",
    "In our iris classification problem, both input variables take real values ($x_i \\in \\mathbb{R}^2$ for $i \\in \\{1, \\dots, n \\}$), whereas the output variable is nominal, taking values from the finite set $\\{setosa, versicolor, virginica\\}$. Hence, the model $f$ we are looking for is a mapping $$f: \\mathbb{R}^2 \\rightarrow \\{setosa, versicolor, virginica\\}.$$\n",
    "\n",
    "### Nearest neighbour classification\n",
    "\n",
    "A very simple technique to derive a classifier model from a given training dataset is the _nearest neighbour algorithm_. It departs from the assumption that instances whose features are highly similar are more likely to have the same label than those with very different features.\\\n",
    "In particular, the _k-nearest neighbors algorithm_ for classification classifies an instance by a plurality vote of its $k$ closest neighbours. If $k=1$, the model applies this idea in its most extreme form: the label is predicted as the label of the closest instance in the training dataset.\\\n",
    "\\\n",
    "In order to select the 'closest' instances in the training set, a suitable measure of distance $d(x_i, x_j)$ between two instances $x_i$ and $x_j$ is used. In our case, we will simply use the Euclidean distance:\n",
    "$$ d_E (x_i, x_j) = \\sqrt{\\sum_{k=1}^p (x_{i,k} - x_{j,k})^2}.$$\n",
    "\n",
    "Using this distance measure, the ($1$-) nearest neighbor algorithm consists of the following steps: \n",
    "1. For an instance with unknown label and known feature vector $\\boldsymbol{x}$, calculate the distance to each instance in the dataset: $d_E(\\boldsymbol{x}, \\boldsymbol{x_i})$ where $i = 1, ... ,n.$\n",
    "2. Select the closest instance and take its label as the prediction for the unknown label.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet downloads all necessary files for this pc-lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-26 15:36:25--  https://raw.githubusercontent.com/tfmortie/mlmust/main/02_knn/abalone.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32955 (32K) [text/plain]\n",
      "Saving to: ‘abalone.csv.1’\n",
      "\n",
      "abalone.csv.1       100%[===================>]  32,18K  --.-KB/s    in 0,002s  \n",
      "\n",
      "2024-01-26 15:36:25 (20,7 MB/s) - ‘abalone.csv.1’ saved [32955/32955]\n",
      "\n",
      "--2024-01-26 15:36:25--  https://raw.githubusercontent.com/tfmortie/mlmust/main/02_knn/iris120.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3359 (3,3K) [text/plain]\n",
      "Saving to: ‘iris120.csv.1’\n",
      "\n",
      "iris120.csv.1       100%[===================>]   3,28K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-01-26 15:36:26 (7,76 MB/s) - ‘iris120.csv.1’ saved [3359/3359]\n",
      "\n",
      "--2024-01-26 15:36:26--  https://raw.githubusercontent.com/tfmortie/mlmust/main/02_knn/irisNA.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 623 [text/plain]\n",
      "Saving to: ‘irisNA.csv.1’\n",
      "\n",
      "irisNA.csv.1        100%[===================>]     623  --.-KB/s    in 0s      \n",
      "\n",
      "2024-01-26 15:36:26 (19,8 MB/s) - ‘irisNA.csv.1’ saved [623/623]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/tfmortie/mlmust/main/02_knn/abalone.csv \n",
    "!wget https://raw.githubusercontent.com/tfmortie/mlmust/main/02_knn/iris120.csv \n",
    "!wget https://raw.githubusercontent.com/tfmortie/mlmust/main/02_knn/irisNA.csv "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 1.1</b>: **Load the dataset iris120.csv in to the memory and select the columns 'Sepal.Length', 'Sepal.Width', and 'Species'. Additionally, load the set of unclassified\n",
    "instances (irisNA.csv) and select the same columns. Both datasets should be loaded as Pandas dataframes.**\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['Sepal.Length', 'Sepal.Width', 'Species']\n",
    "# load the two datasets and select the respective columns\n",
    "\n",
    "# iris120 as the training set\n",
    "iris120 = pd.read_csv(\"iris120.csv\")\n",
    "iris120 = iris120.loc[:, cols]\n",
    "\n",
    "# irisNA as the test set\n",
    "irisNA = pd.read_csv(\"irisNA.csv\")\n",
    "irisNA = irisNA.loc[:, cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width Species\n",
       "0           5.1          3.5  setosa\n",
       "1           4.9          3.0  setosa\n",
       "2           4.7          3.2  setosa\n",
       "3           4.6          3.1  setosa\n",
       "4           5.0          3.6  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris120.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Species\n",
       "0           5.4          3.9      NaN\n",
       "1           5.0          3.4      NaN\n",
       "2           5.8          4.0      NaN\n",
       "3           5.4          3.9      NaN\n",
       "4           4.4          3.0      NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisNA.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 1.2</b>: **Implement the nearest neighbour algorithm (as explained above) for the iris problem in a function called nn_iris_predict.\n",
    "Use this function to predict the species of unknown flowers in the dataset irisNA.csv. Make sure\n",
    "your function has the structure given below. \\\n",
    "(Here, _new_obs_features_ is an array or dataframe containing the two features for one specific new instance, and _train_dataset_ is the dataframe containing the features and labels (i.e. _iris120_ as initialized above)).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_iris_predict(new_obs_features, train_dataset):\n",
    "\n",
    "    # extract features (i.e. first two columns) from training dataset to calculate the distances\n",
    "    train_features = train_dataset.iloc[:, :2]\n",
    "\n",
    "    # extract species (labels) of training dataset in a separate variable\n",
    "    train_labels = train_dataset.iloc[:, 2]\n",
    "\n",
    "    # create a variable 'dist_euc' which is an array containing the euclidean distance of\n",
    "    # the new instance to all instances (rows) in the training data set\n",
    "    dist_euc = ((train_features - new_obs_features) ** 2).sum(axis=1) ** .5\n",
    "\n",
    "    # extract index of nearest neighbor (i.e. the index of the smallest value in the array 'dist_euc')\n",
    "    nn_ind = np.argmin(dist_euc)\n",
    "\n",
    "    # extract species label on the respective index\n",
    "    nn_label = train_labels.iloc[nn_ind]\n",
    "\n",
    "    return nn_label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the algorithm to predict the species of the first instance of the unknown flowers dataset irisNA.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract features of the first instance in the test set\n",
    "new_obs_features = irisNA.iloc[0, :2]\n",
    "\n",
    "# predict the species of the first instance in the test set\n",
    "nn_iris_predict(new_obs_features, iris120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nearest neighbor algorithm for regression\n",
    "In the previous section, the output $y$ was a nominal variable (i.e. one specific class from a discrete set of possible classes). When the output is real-valued ($y \\in \\mathbb{R}$), the prediction problem is called a _regression problem_. \\\n",
    "\\\n",
    "As with nominal outputs, the nearest neighbor algorithm can also be applied in this case. It is identical to the one used in the classification task, where the predicted label now is the real-valued label of the instance closest in the training dataset.\n",
    "\n",
    "### Data preprocessing\n",
    "For solving the next task, some more elementary data preprocessing steps need to be introduced.\n",
    "\n",
    "#### Dummy encoding of nominal variables \n",
    "Often, the features in a dataset are not numerical, but nominal or ordinal. In this case, to be still able to use algorithms relying on numerical values such as the nearest neighbor algorithm, we can use _dummy encodings_ for each nominal variable.\\\n",
    "\\\n",
    "In dummy encodings, a variable (feature) $x^i$ that can take $k$ possible (nominal) values is replaced by $k$ new binary variables (features). As an example, consider a dataset where one feature (i.e. $x^1$) displays the weather status, taking the $3$ possible values $\\{Sunny, Overcast, Rainy\\}$. Each of these values can be represented by a dummy variable: $x^{1a}$, $x^{1b}$ and $x^{1c}$, with values \n",
    "\n",
    "$$\\begin{equation*}\n",
    "x^{1a} = \\begin{cases} 1,\\quad if \\quad x^1 = \"Sunny\" \\\\\n",
    "                     0, \\quad otherwise \\end{cases}\n",
    "\\end{equation*}$$\n",
    "\n",
    "$$\\begin{equation*}\n",
    "x^{1b} = \\begin{cases} 1,\\quad if \\quad x^1 = \"Overcast\" \\\\\n",
    "                     0, \\quad otherwise \\end{cases}\n",
    "\\end{equation*}$$\n",
    "\n",
    "$$\\begin{equation*}\n",
    "x^{1c} = \\begin{cases} 1,\\quad if \\quad x^1 = \"Rainy\" \\\\\n",
    "                     0, \\quad otherwise \\end{cases}\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will look at the task of predicting the age of abalone, a type of marine snail.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/LivingAbalone.JPG/2560px-LivingAbalone.JPG\" width=500>\n",
    "\n",
    "The dataset _abalone.csv_ contains measurements of physical properties of several abalone specimen. By using these physical properties, the aim is to build a predictive model for the age of these animals (more information concerning this dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)). In the following example, we replace the nominal variable 'sex' by three dummy variables (i.e. as many as the values it takes). This can be done by using one of several functions that are provided in Python, such as the [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function from Pandas, or the [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) function from Scikit-learn. First, we create the dummy variables, then we concatenate them with the original dataset and finally we remove the original variable form the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sex  length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
      "0   I   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
      "1   F   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
      "2   F   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
      "3   I   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
      "4   I   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
      "\n",
      "   shellWeight  age  \n",
      "0       0.3595    9  \n",
      "1       0.1700   14  \n",
      "2       0.2950    9  \n",
      "3       0.0750    8  \n",
      "4       0.1950    9  \n"
     ]
    }
   ],
   "source": [
    "# read abalone dataset as pandas dataframe\n",
    "abalone = pd.read_csv(\"abalone.csv\")\n",
    "print(abalone.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F  I  M\n",
      "0  0  1  0\n",
      "1  1  0  0\n",
      "2  1  0  0\n",
      "3  0  1  0\n",
      "4  0  1  0\n"
     ]
    }
   ],
   "source": [
    "# create dummy variables for the \"sex\" feature\n",
    "dummies = pd.get_dummies(abalone.sex)\n",
    "print(dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
      "0   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
      "1   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
      "2   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
      "3   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
      "4   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
      "\n",
      "   shellWeight  age  F  I  M  \n",
      "0       0.3595    9  0  1  0  \n",
      "1       0.1700   14  1  0  0  \n",
      "2       0.2950    9  1  0  0  \n",
      "3       0.0750    8  0  1  0  \n",
      "4       0.1950    9  0  1  0  \n"
     ]
    }
   ],
   "source": [
    "# concatenate the dummy variables to the original dataframe\n",
    "abalone_dummy_encoded = pd.concat([abalone, dummies], axis=1)\n",
    "# remove the original 'sex' column\n",
    "abalone_dummy_encoded = abalone_dummy_encoded.drop(['sex'], axis=1)\n",
    "print(abalone_dummy_encoded.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the data\n",
    "In realistic datasets, most features have different means and standard deviations. For the nearest\n",
    "neighbour algorithm, it can easily be seen that features with a high standard deviation will be more\n",
    "influential than features with a lower standard deviation. In most cases, this is unwanted since it\n",
    "is not known in advance which features are most important. To overcome this problem, features\n",
    "are often _standardized_, a scaling method where the values are centered around the (sample) mean with a unit standard deviation.\\\n",
    "The standardized version of a feature $x^i$ can be obtained as   \n",
    "$$ x^{i'} = \\frac{x^i - \\mu_i}{\\sigma_i},$$    \n",
    "where $\\mu_i$ and $\\sigma_i$ represent the sample mean and standard deviation of $\\boldsymbol{x^i}$.\n",
    "The [Scaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) function of Scikit-learn can be used to perform this standardization.\n",
    "\n",
    "Notice that what we're doing here (ie. scaling the training and test set together in one operation) is usually considered bad practice as it will leak data from test to train and hence bias model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# split abalone_dummy_encoded into features and labels\n",
    "y = abalone_dummy_encoded['age'].values # keep target variable\n",
    "X = abalone_dummy_encoded.drop(['age'], axis=1) # remove it from the features\n",
    "\n",
    "# scale the features: will return a numpy array\n",
    "X = scale(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting and prediction quality\n",
    "\n",
    "In order to test the performance of a nearest neighbor model, one can split a dataset in a training $\\mathcal{D}_{train}$ and test set $\\mathcal{D}_{test}$. The training set is used to train a model and the test set is used to evaluate its performance. For a regression problem, one typically uses the _mean squared error_ (MSE) to evaluate the quality of the model. For any dataset $\\mathcal{D}$, it is calculated as follows:\n",
    "$$ \\text{MSE} = \\frac{1}{|\\mathcal{D} |} \\sum_{\\boldsymbol{x_i} \\in \\mathcal{D}}(f(\\boldsymbol{x_i}) - y_i)^2$$\n",
    "The metric above can be computed for the training set and test set (if labels are available) for comparison.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 2.1</b>: **To prepare the dataset for the following exercise, split the dataset in a training set (80% of the data) and test set (20% of the data). See the documentation of the function [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# name the variables for train/test features and labels X_train, y_train, X_test, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-nearest neighbors for regression\n",
    "\n",
    "Classifying an instance only by the (one) nearest neighbor to it might not always be very accurate. Hence, a simple extension of the nearest neighbor algorithm consists of taking not only one, but multiple neighbors into account.\n",
    "\n",
    "To this end, let $N_k(\\boldsymbol{x}) \\subset \\mathcal{D}$ be the $k$ nearest neighbors of an instance with feature vector $\\boldsymbol{x}$.\n",
    "\n",
    "For __classification__:\n",
    "- set $f(\\boldsymbol{x})$ to be the _majority vote_ of its neighbors, i.e. the label that occurs most often in $N_k(\\boldsymbol{x})$\n",
    "\n",
    "For __regression__:\n",
    "- set $f(\\boldsymbol{x})$ to be the _average_ of $N_k(\\boldsymbol{x})$, i.e. $$ f(\\boldsymbol{x}) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i.$$\n",
    "\n",
    "### K-nearest neighbors in Python\n",
    "\n",
    "As with the $1$-nearest neighbor algorithm, it is possible to implement a version of the k-nearest neighbors algorithm in Python from scratch. However, to simplify things, an alternative is to use a pre-implemented version of the algorithm. Here, we use the [Scikit-learn](http://scikit-learn.org/stable/) library again, which has an implementation of \n",
    "the $k$-nearest neighbor algorithm availbale. You can load and see more info about the usage of this function by going to the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) or by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsRegressor in module sklearn.neighbors._regression:\n",
      "\n",
      "class KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      " |  KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      " |  \n",
      " |  Regression based on k-nearest neighbors.\n",
      " |  \n",
      " |  The target is predicted by local interpolation of the targets\n",
      " |  associated of the nearest neighbors in the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <regression>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.9\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, default=5\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      " |      Weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |      Uniform weights are used by default.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, default=30\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : int, default=2\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : str or callable, default='minkowski'\n",
      " |      Metric to use for distance computation. Default is \"minkowski\", which\n",
      " |      results in the standard Euclidean distance when p = 2. See the\n",
      " |      documentation of `scipy.spatial.distance\n",
      " |      <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n",
      " |      the metrics listed in\n",
      " |      :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n",
      " |      values.\n",
      " |  \n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square during fit. X may be a :term:`sparse graph`, in which\n",
      " |      case only \"nonzero\" elements may be considered neighbors.\n",
      " |  \n",
      " |      If metric is a callable function, it takes two arrays representing 1D\n",
      " |      vectors as inputs and must return one value indicating the distance\n",
      " |      between those vectors. This works for Scipy's metrics, but is less\n",
      " |      efficient than passing the metric name as a string.\n",
      " |  \n",
      " |  metric_params : dict, default=None\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  effective_metric_ : str or callable\n",
      " |      The distance metric to use. It will be same as the `metric` parameter\n",
      " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      " |      'minkowski' and `p` parameter set to 2.\n",
      " |  \n",
      " |  effective_metric_params_ : dict\n",
      " |      Additional keyword arguments for the metric function. For most metrics\n",
      " |      will be same with `metric_params` parameter, but may also contain the\n",
      " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      " |      'minkowski'.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_samples_fit_ : int\n",
      " |      Number of samples in the fitted data.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n",
      " |  RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n",
      " |  KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n",
      " |  RadiusNeighborsClassifier : Classifier implementing\n",
      " |      a vote among neighbors within a given radius.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      " |     different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      " |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      " |  >>> neigh.fit(X, y)\n",
      " |  KNeighborsRegressor(...)\n",
      " |  >>> print(neigh.predict([[1.5]]))\n",
      " |  [0.5]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsRegressor\n",
      " |      sklearn.neighbors._base.KNeighborsMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.neighbors._base.NeighborsBase\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the k-nearest neighbors regressor from the training dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : KNeighborsRegressor\n",
      " |          The fitted k-nearest neighbors regressor.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the target for the provided data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n",
      " |          Target values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Find the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors required for each sample. The default is the\n",
      " |          value passed to the constructor.\n",
      " |      \n",
      " |      return_distance : bool, default=True\n",
      " |          Whether or not to return the distances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True.\n",
      " |      \n",
      " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NearestNeighbors\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples)\n",
      " |      NearestNeighbors(n_neighbors=1)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      " |      (array([[0.5]]), array([[2]]))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False)\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |          For ``metric='precomputed'`` the shape should be\n",
      " |          (n_queries, n_indexed). Otherwise the shape should be\n",
      " |          (n_queries, n_features).\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors for each sample. The default is the value\n",
      " |          passed to the constructor.\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are distances between points, type of distance\n",
      " |          depends on the selected metric parameter in\n",
      " |          NearestNeighbors class.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      " |          `n_samples_fit` is the number of samples in the fitted data.\n",
      " |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      " |          The matrix is of CSR format.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
      " |          of Neighbors for points in X.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X)\n",
      " |      NearestNeighbors(n_neighbors=2)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[1., 0., 1.],\n",
      " |             [0., 1., 1.],\n",
      " |             [1., 0., 1.]])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "help(neighbors.KNeighborsRegressor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the help window, in Scikit-learn, an estimator for classification/regression is a _Python object_ that implements the methods _fit(X, y)_ and _predict(T)_.\n",
    "\n",
    "The constructor of an estimator takes as arguments the parameters of the model (in our case the basic parameters are the number of neighbours and the distance metric) and is used to instantiate an object. Here, we set the number of neighbors to $k=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors, metric='euclidean')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our estimator instance `knn`. It now must be fitted to the data, that is, it must learn from the data. This is done by passing our training set to the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsRegressor(metric='euclidean')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the estimator is fitted to the training data, the estimator can be used to predict the age of the first example in our test dataset. Remember, this is done by comparing the features of the first example to the features of all training samples, followed by determining the closest neighbors. The prediction is then obtained by averaging the age of the neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_1 = knn.predict(X_test[0].reshape(1,-1))\n",
    "y_pred_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can calculate the mean squared error of the prediction by manual calculation or by using the [mean_squared_error()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function in Scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [11.2]\n",
      "1.4399999999999984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(y_test[0], y_pred_1)\n",
    "print(mean_squared_error(y_pred_1, y_test[0].reshape(1, -1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 3.1</b>: **Build a 3-nearest neighbors regressor in a similar manner as before. Predict the age for all training samples and test samples and compute the mean squared error. Compare the training and test error.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.706746031746032\n",
      "6.534126984126983\n"
     ]
    }
   ],
   "source": [
    "# now use k=3 neighbors for training\n",
    "n_neighbors = 3\n",
    "# Instantiate the knn object\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors, metric='euclidean')\n",
    "# Fit\n",
    "knn.fit(X_train, y_train)\n",
    "# Predict train\n",
    "y_pred_train = knn.predict(X_train)\n",
    "# Predict test\n",
    "y_pred_test = knn.predict(X_test)\n",
    "# print MSE train\n",
    "print(mean_squared_error(y_pred_train, y_train))\n",
    "# print MSE test\n",
    "print(mean_squared_error(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $k$ in the $k$-nearest neighbors algorithm is called a hyperparameter. A parameter of that kind is typically choosen by the user (i.e. it is not learned) and depends on the prediction task and dataset. This is illustrated in the next exercise. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 3.2</b>: **Repeat the previous exercise by considering different values for $k$. You can consider a range between 1 and 50 and a step size of 3. For each $k$, calculate and plot the training and test error of the resulting model.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x111b25310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use lists to store the results obtained from every iteration\n",
    "results_train = []\n",
    "results_test = [] \n",
    "for i in range(1, 50, 3):\n",
    "    # instantiate the model\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=i, metric='euclidean')\n",
    "    # Fit\n",
    "    knn.fit(X_train, y_train)\n",
    "    # Predict train\n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    # Predict test\n",
    "    y_pred_test = knn.predict(X_test)\n",
    "    # calculate and store the MSE train\n",
    "    mse_train = mean_squared_error(y_pred_train, y_train)\n",
    "    results_train.append(mse_train)\n",
    "    # calculate and store MSE test\n",
    "    mse_test = mean_squared_error(y_pred_test, y_test)\n",
    "    results_test.append(mse_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, 50, 3), results_train) # training MSEs\n",
    "plt.plot(range(1, 50, 3), results_test) # test MSEs\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$k$')\n",
    "plt.legend(['Train MSE', 'Test MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
