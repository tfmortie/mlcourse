{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixjoojZvfEhh"
      },
      "source": [
        "# PC Lab 2: Nearest neighbour and data preprocessing\n",
        "Predictive modelling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH1K-8KXfEhn"
      },
      "source": [
        "## Introduction\n",
        "In our previous lab session, we explored the iris dataset. In this dataset, there are no missing values in the variable \"Species\". But what if there are new observations without a value in the \"Species\" variable? So, imagine the case that for a 'new' iris flower we know the values for the other columns (sepal/petal length and height) but\n",
        "it is unknown to which of the three species it belongs to (dataset irisNA.csv). A natural task would be\n",
        "to try to guess to which species each of the new flowers belongs to. This task\n",
        "(or problem) is called a classification problem in machine learning. In this practical exercise session, a first\n",
        "simple algorithm that provides an answer to this problem is described.\n",
        "### Notations and vocabulary\n",
        "In the iris dataset (iris120.csv), each instance (each flower is an instance) is described by five properties:\n",
        "the species it belongs to, the width of its petals, the length of its petals, the width of its sepals and the\n",
        "length of its sepals. In this PC-lab, for simplicity, only species, sepal length and sepal width will be used.\n",
        "These properties can be seen as variables, and for a given flower, each of these variables takes a specific\n",
        "value. In a classification setting, the aim is to predict the value of one of the variables (here the species),\n",
        "based on the value of the other variables (here petal width and length). The variable of which the values\n",
        "have to be predicted is called the output variable and the variables used to make this prediction are called\n",
        "the input variables or features. A dataset consists of a set of observations of input-output couples $(\\boldsymbol{x}, y)$. In this dataset, the observed values\n",
        "for the features of the $i$-th instance are denoted $\\boldsymbol{x_i}$ $= (x_{i1}, ... , x_{ip})^T$ , where $p$ the number of features, and the observed value of its output\n",
        "is denoted $y_i$. Using this notation, a training dataset $T$ containing $n$ instances can be written as $$T = \\{(\\boldsymbol{x_1}, y_1), ... , (\\boldsymbol{x_n}, y_n)\\}.$$\n",
        "Using this dataset, we will try to build a model (generally denoted $f$) that is able to predict the value of the\n",
        "output variable, based on the value of the input variables. When this output variable is nominal, this process\n",
        "is called a classification problem.    \n",
        "In the iris problem, both input variables take real values $(\\boldsymbol{x_i} \\in \\mathbb{R}^2)$. The output variable, however, is\n",
        "nominal, it takes values from a finite set $\\{setosa, versicolor, virginica\\}$. Because of this, the model $\\textit{f}$ we are\n",
        "looking for is one which performs a mapping $$\\textit{f} : \\mathbb{R}^2 \\rightarrow  \\{setosa, versicolor, virginica\\}.$$   \n",
        "### Nearest neighbour for classification\n",
        "Several techniques exist that are capable of deriving classification models from data. A very simple one is\n",
        "the nearest neighbour model. This model departs from the assumption that instances whose features are\n",
        "highly similar, are likely to have the same labels. The one nearest neighbour (1-NN) model applies this\n",
        "idea in its most extreme form: the label for an instance (with unknown label) is predicted as the label of the\n",
        "closest instance in the training dataset.   \n",
        "To be able to select the ‘closest’ instance in the training dataset, a distance measure has to be defined. In\n",
        "this text, we will use $d(\\boldsymbol{x_i}, \\boldsymbol{x_j})$ to denote the distance between two feature vectors $\\boldsymbol{x_i}$ and $\\boldsymbol{x_j}$. As a simple\n",
        "distance measure, the Euclidean distance can be used\n",
        "$$d_E(\\boldsymbol{x_i}, \\boldsymbol{x_j}) = \\sqrt{\\sum_{k=1}^{p} (x_{i,k} - x_{j,k})^2}$$\n",
        "Using this distance function, the nearest neighbour algorithm performs the following steps:\n",
        "1. For an instance with unknown label and known feature vector $\\boldsymbol{x}$, calculate the distance to each instance in the dataset: $d_E(\\boldsymbol{x}, \\boldsymbol{x_i})$ where $i = 1, ... ,n.$\n",
        "2. Select the closest instance and take its label as the prediction for the unknown label."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first codeblock downloads all the necessary data for this pc-lab."
      ],
      "metadata": {
        "id": "RfMArc34t4_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/BioML-UGent/MLLS/main/02_knn/abalone.csv\n",
        "!wget https://raw.githubusercontent.com/BioML-UGent/MLLS/main/02_knn/iris120.csv\n",
        "!wget https://raw.githubusercontent.com/BioML-UGent/MLLS/main/02_knn/irisNA.csv"
      ],
      "metadata": {
        "id": "IE_U9a1Ot4TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXJNJBhcfEhr"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Load the dataset iris120.csv in to the memory and select the columns 'Sepal.Length', 'Sepal.Width', and 'Species'. Additionally, load the set of unclassified\n",
        "instances (irisNA.csv) and select the same columns. Both datasets should be loaded as data frames. **\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "7LJ85LnpfEhu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris120 = pd.read_csv('iris120.csv') # ... load in dataset iris120.csv\n",
        "iris120 = iris120[['Sepal.Length', 'Sepal.Width', 'Species']] # ... select the right columns\n",
        "\n",
        "irisNA = pd.read_csv('irisNA.csv') # ... repeat for irisNA.csv\n",
        "irisNA = irisNA[['Sepal.Length', 'Sepal.Width', 'Species']] # ... repeat for irisNA.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris120.head()"
      ],
      "metadata": {
        "id": "3G3zeHSb1Dxa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "d68571ef-e4f5-4db9-f961-eb3ffac22f3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sepal.Length  Sepal.Width Species\n",
              "0           5.1          3.5  setosa\n",
              "1           4.9          3.0  setosa\n",
              "2           4.7          3.2  setosa\n",
              "3           4.6          3.1  setosa\n",
              "4           5.0          3.6  setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f7a3101-db22-498b-8e9c-5e2fdd0e18d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sepal.Length</th>\n",
              "      <th>Sepal.Width</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f7a3101-db22-498b-8e9c-5e2fdd0e18d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f7a3101-db22-498b-8e9c-5e2fdd0e18d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f7a3101-db22-498b-8e9c-5e2fdd0e18d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aeeddbff-fce0-42a2-be26-e1a107466d83\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aeeddbff-fce0-42a2-be26-e1a107466d83')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aeeddbff-fce0-42a2-be26-e1a107466d83 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "irisNA.head()"
      ],
      "metadata": {
        "id": "pKsIKSYh1EIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1ba3b718-cbb5-4782-aad9-3ce6277c0cd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sepal.Length  Sepal.Width  Species\n",
              "0           5.4          3.9      NaN\n",
              "1           5.0          3.4      NaN\n",
              "2           5.8          4.0      NaN\n",
              "3           5.4          3.9      NaN\n",
              "4           4.4          3.0      NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27101549-bcf3-4069-8d58-4fe04515c449\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sepal.Length</th>\n",
              "      <th>Sepal.Width</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27101549-bcf3-4069-8d58-4fe04515c449')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-27101549-bcf3-4069-8d58-4fe04515c449 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-27101549-bcf3-4069-8d58-4fe04515c449');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d0064a87-1e0d-494c-a2e1-e52664351f06\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0064a87-1e0d-494c-a2e1-e52664351f06')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d0064a87-1e0d-494c-a2e1-e52664351f06 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zczv03YfEhv"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Implement the nearest neighbour algorithm for the iris problem in a function called nnIrisPredict.\n",
        "Use this function to predict the species of unknown flowers irisNA.csv in the dataset. Make sure\n",
        "your function has the following structure:**\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_observation_features =  # isolate the first two columns\n",
        "trainDataset = iris120"
      ],
      "metadata": {
        "id": "G47xy38Cgkn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zP8apShlfEhw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88e22369-d502-4923-d4ee-d1af2dda6301"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'virginica'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "def nnIrisPredict(new_observation_features, trainDataset):\n",
        "    # create a variable 'dist' containing the euclidean distance of the\n",
        "    # new instance to all instances (rows) in the training dataset:\n",
        "    dist =\n",
        "    # what is the index of the nearest neighbor 'nn' in our training dataset:\n",
        "    nn =\n",
        "    # Extract the species label of that nearest neighbor\n",
        "    # and return that as prediction:\n",
        "    label =\n",
        "    return label\n",
        "\n",
        "\n",
        "# call the function\n",
        "nnIrisPredict(new_observation_features, trainDataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-BvbEnfEhx"
      },
      "source": [
        "where trainDataset is a data frame (containing flowers with known species label) with columns\n",
        "\"Sepal.Length\", \"Sepal.Width\" and \"Species\". featuresNewInstance is a vector with the sepal\n",
        "length in the first position and the sepal width in the second position. Label should be one of the\n",
        "strings \"setosa\", \"versicolor\" and \"virginica\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXpRMd6CfEhy"
      },
      "source": [
        "## The nearest neighbour algorithm for regression\n",
        "In the previous section, the output was a nominal variable (no numerical values, specific classes). When the output is real-valued, the prediction problem is called a regression problem. As with nominal outputs, the nearest neighbour algorithm can\n",
        "be used to predict the output variable of unlabeled instances. The algorithm is identical to the one for\n",
        "classification, however, the prediction will be the real-valued label of the closest instance in the training\n",
        "dataset instead of its class label.\n",
        "\n",
        "## Data preprocessing\n",
        "In this section, some elementary data preprocessing steps are described.\n",
        "### Dummy encoding of nominal variables\n",
        "The basic nearest neighbour algorithm implemented in the previous assignment can only be used with\n",
        "numerical features. However, often types of variables such as nominal variables or ordinal variables are\n",
        "present. A simple solution to this problem exists in using a dummy encoding for each nominal variable.\n",
        "When a variable $\\boldsymbol{x^i}$ is nominal with $k$ values, it is replaced by $k$ new binary variables. As an imaginary example,\n",
        "consider a dataset with a feature containing weather status. This feature ($\\boldsymbol{x^1}$) could for example contain three values: Sunny, Overcast and Rainy.\n",
        "Each of these values could be represented by a dummy variable: $\\boldsymbol{x^{1a}}$, $\\boldsymbol{x^{1b}}$ and $\\boldsymbol{x^{1c}}$ with the following values:   \n",
        "  *  $x^{1a} = 1$ if $x^1 = \"Sunny\"$ and  $x^{1a} = 0$ otherwise\n",
        "  * $x^{1b} = 1$ if $x^1 = \"Overcast\"$ and $x^{1b} = 0$ otherwise\n",
        "  * $x^{1c} = 1$ if $x^1 = \"Rainy\"$ and $x^{1c} = 0$ otherwise\n",
        "  \n",
        "See the following example in python. In this example, we use  the Abalone dataset (abaloneTrain700.csv) which contains measurements of physical properties of several abalone (an edible sea snail) specimen. Using these physical properties, the aim is to build a predictive model for the age of these animals (more information concerning this dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)). In the following example, we replace the nominal variable 'sex' with 3 dummy variables (as many as the values it takes). In python, there are functions such as the [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function which are used for this purpose. (Another option is scikit-learn's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)) So firstly, we create the dummy variables, then we concatenate them with the original dataset and finally we remove the original variable form the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8KcIkTvyfEhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b793262-b528-4c2c-c6c5-758e00971b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  sex  length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
            "0   I   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
            "1   F   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
            "2   F   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
            "3   I   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
            "4   I   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
            "\n",
            "   shellWeight  age  \n",
            "0       0.3595    9  \n",
            "1       0.1700   14  \n",
            "2       0.2950    9  \n",
            "3       0.0750    8  \n",
            "4       0.1950    9  \n",
            "   F  I  M\n",
            "0  0  1  0\n",
            "1  1  0  0\n",
            "2  1  0  0\n",
            "3  0  1  0\n",
            "4  0  1  0\n",
            "   length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
            "0   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
            "1   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
            "2   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
            "3   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
            "4   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
            "\n",
            "   shellWeight  age  F  I  M  \n",
            "0       0.3595    9  0  1  0  \n",
            "1       0.1700   14  1  0  0  \n",
            "2       0.2950    9  1  0  0  \n",
            "3       0.0750    8  0  1  0  \n",
            "4       0.1950    9  0  1  0  \n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "abalone = pd.read_csv('abalone.csv')\n",
        "print(abalone.head())\n",
        "# create dummies\n",
        "dummies = pd.get_dummies(abalone.sex)\n",
        "print(dummies.head())\n",
        "# concatenate them with the dataset\n",
        "abalone_with_dummies = pd.concat([abalone, dummies], axis=1)\n",
        "# remove the original sex column\n",
        "abalone_with_dummies = abalone_with_dummies.drop(['sex'], axis=1)\n",
        "print(abalone_with_dummies.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A8puB8nfEh0"
      },
      "source": [
        "### Missing values\n",
        "Missing values are commonly encountered in data mining studies. Often, missing values are imputed\n",
        "(replaced by a value). Several techniques exist to choose this value. A simple, but often used method\n",
        "is mean imputation. Here, each missing value is replaced by the mean of the observed values for\n",
        "that variable. More advanced methods exist of building separate models to predict the missing\n",
        "values.\n",
        "When implementing the mean imputation, the [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) of scikit-learn library might be handy. The PC labs in this practical have no missing values however:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(abalone_with_dummies).any()"
      ],
      "metadata": {
        "id": "NRrYEy-u6wFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yR7l-8fEh1"
      },
      "source": [
        "### Standardizing the data\n",
        "In realistic datasets, most features have different means and standard deviations. For the nearest\n",
        "neighbour algorithm, it can easily be seen that features with a high standard deviation will be more\n",
        "influential than features with a lower standard deviation. In most cases, this is unwanted since it\n",
        "is not known in advance which features are most important. To overcome this problem, features\n",
        "are often standardized. The standardized version of $x^i$ can be obtained as   \n",
        "$$\\frac{x^i - \\mu_i}{\\sigma_i}$$    \n",
        "where $\\mu_i$ and $\\sigma_i$ represent the sample mean and standard deviation of $\\boldsymbol{x^i}$.\n",
        "The [Scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) of scikit-learn can be used to perform this standardizing.\n",
        "\n",
        "Notice that we're doing here (scaling the training and test set together in one operation) is usually considered bad practice as it will leak data from train to test and hence bias model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M1gEg0bnfEh2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import scale\n",
        "\n",
        "y = abalone_with_dummies['age'].values # keep the target variable\n",
        "X = abalone_with_dummies.drop(['age'], axis=1) # remove it from the feature set\n",
        "X = scale(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55n8InUsfEh7"
      },
      "source": [
        "\n",
        "\n",
        "### Data splitting and Prediction quality\n",
        "To test the performance of the nearest neighbour algorithm, an option is to use test data with\n",
        "known labels and to compare the predictions with these labels. In this case, we have two different\n",
        "datasets, a training set T and a test set T$^*$. In case of a regression problem, the mean of squared\n",
        "residuals is commonly used to evaluate the quality of a model. This measure is calculated as follows:   \n",
        "1. Mean of squared residuals on test set:\n",
        "$$Err_{T^*} = \\frac{1}{|T^*|}\\sum_{\\boldsymbol{x_i} \\in T^*} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
        "2. Additionally, the error on the training data itself can be computed:   \n",
        "$$Err_{T} = \\frac{1}{|T|}\\sum_{\\boldsymbol{x_i} \\in T} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
        "\n",
        "Naturally, it is desirable to keep the mean of squared residuals as small as possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi0sPwRufEh2"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **To prepare the dataset for the following exercise, split the dataset in a portion (80%) we will use to train on, and a portion we will use to predict on (20%) (test set). See the documentation of scikit learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for more info on how to do this.**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cr8fjikmfEh2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# you should get four variables named: X_train, y_train, X_test & y_test\n",
        "X_train, X_test, y_train, y_test ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GHn7LXLfEh3"
      },
      "source": [
        "## k-nearest-neighbours\n",
        "A simple extension of the nearest neighbour algorithm consists of taking more than only the nearest\n",
        "neighbour into account. Let $N_k(\\boldsymbol{x}) \\subset T$ be the k nearest neighbours of an instance with feature\n",
        "vector $\\boldsymbol{x}$. The k-nearest neighbour prediction $Y(\\boldsymbol{x})$ can be determined as follows\n",
        "1. For classification problems:  $Y(\\boldsymbol{x})$ is set as the label that occurs most often in $N_k(\\boldsymbol{x})$ (the\n",
        "mode).\n",
        "2. For regression problems, the average of the $k$ nearest outputs can be taken $Y(\\boldsymbol{x}) = \\frac{1}{k}\\sum_{\\boldsymbol{x_i} \\in N_k(\\boldsymbol{x})} y_i$    \n",
        "\n",
        "### k-nearest-neighbours in python\n",
        "As with the 1-nearest neighbour algorithm, it is possible to implement a version of the k-nearest neighbours\n",
        "algorithm in python. However, an alternative is to use a pre-implemented version of this\n",
        "algorithm. For most popular machine learning algorithms, these functions are packed in specific python\n",
        "“packages\". An implementation of\n",
        "the k-nearest-neighbours algorithm is available in the [scikit-learn](http://scikit-learn.org/stable/) library (as well).\n",
        "You can load and see more info about the usage of this function by typing: (or by going to the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iwPa3SamfEh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4dcea6f-0fce-467e-cede-5a7fa876241c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class KNeighborsRegressor in module sklearn.neighbors._regression:\n",
            "\n",
            "class KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
            " |  KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
            " |  \n",
            " |  Regression based on k-nearest neighbors.\n",
            " |  \n",
            " |  The target is predicted by local interpolation of the targets\n",
            " |  associated of the nearest neighbors in the training set.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <regression>`.\n",
            " |  \n",
            " |  .. versionadded:: 0.9\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  n_neighbors : int, default=5\n",
            " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
            " |  \n",
            " |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
            " |      Weight function used in prediction.  Possible values:\n",
            " |  \n",
            " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
            " |        are weighted equally.\n",
            " |      - 'distance' : weight points by the inverse of their distance.\n",
            " |        in this case, closer neighbors of a query point will have a\n",
            " |        greater influence than neighbors which are further away.\n",
            " |      - [callable] : a user-defined function which accepts an\n",
            " |        array of distances, and returns an array of the same shape\n",
            " |        containing the weights.\n",
            " |  \n",
            " |      Uniform weights are used by default.\n",
            " |  \n",
            " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
            " |      Algorithm used to compute the nearest neighbors:\n",
            " |  \n",
            " |      - 'ball_tree' will use :class:`BallTree`\n",
            " |      - 'kd_tree' will use :class:`KDTree`\n",
            " |      - 'brute' will use a brute-force search.\n",
            " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
            " |        based on the values passed to :meth:`fit` method.\n",
            " |  \n",
            " |      Note: fitting on sparse input will override the setting of\n",
            " |      this parameter, using brute force.\n",
            " |  \n",
            " |  leaf_size : int, default=30\n",
            " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
            " |      speed of the construction and query, as well as the memory\n",
            " |      required to store the tree.  The optimal value depends on the\n",
            " |      nature of the problem.\n",
            " |  \n",
            " |  p : int, default=2\n",
            " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
            " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
            " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
            " |  \n",
            " |  metric : str or callable, default='minkowski'\n",
            " |      The distance metric to use for the tree.  The default metric is\n",
            " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
            " |      metric. See the documentation of :class:`DistanceMetric` for a\n",
            " |      list of available metrics.\n",
            " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
            " |      must be square during fit. X may be a :term:`sparse graph`,\n",
            " |      in which case only \"nonzero\" elements may be considered neighbors.\n",
            " |  \n",
            " |  metric_params : dict, default=None\n",
            " |      Additional keyword arguments for the metric function.\n",
            " |  \n",
            " |  n_jobs : int, default=None\n",
            " |      The number of parallel jobs to run for neighbors search.\n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |      Doesn't affect :meth:`fit` method.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  effective_metric_ : str or callable\n",
            " |      The distance metric to use. It will be same as the `metric` parameter\n",
            " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
            " |      'minkowski' and `p` parameter set to 2.\n",
            " |  \n",
            " |  effective_metric_params_ : dict\n",
            " |      Additional keyword arguments for the metric function. For most metrics\n",
            " |      will be same with `metric_params` parameter, but may also contain the\n",
            " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
            " |      'minkowski'.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  n_samples_fit_ : int\n",
            " |      Number of samples in the fitted data.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n",
            " |  RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n",
            " |  KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n",
            " |  RadiusNeighborsClassifier : Classifier implementing\n",
            " |      a vote among neighbors within a given radius.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
            " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
            " |  \n",
            " |  .. warning::\n",
            " |  \n",
            " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
            " |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
            " |     different labels, the results will depend on the ordering of the\n",
            " |     training data.\n",
            " |  \n",
            " |  https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> X = [[0], [1], [2], [3]]\n",
            " |  >>> y = [0, 0, 1, 1]\n",
            " |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
            " |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
            " |  >>> neigh.fit(X, y)\n",
            " |  KNeighborsRegressor(...)\n",
            " |  >>> print(neigh.predict([[1.5]]))\n",
            " |  [0.5]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      KNeighborsRegressor\n",
            " |      sklearn.neighbors._base.KNeighborsMixin\n",
            " |      sklearn.base.RegressorMixin\n",
            " |      sklearn.neighbors._base.NeighborsBase\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y)\n",
            " |      Fit the k-nearest neighbors regressor from the training dataset.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
            " |          Training data.\n",
            " |      \n",
            " |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
            " |          Target values.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : KNeighborsRegressor\n",
            " |          The fitted k-nearest neighbors regressor.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict the target for the provided data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
            " |          Test samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n",
            " |          Target values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
            " |  \n",
            " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
            " |      Find the K-neighbors of a point.\n",
            " |      \n",
            " |      Returns indices of and distances to the neighbors of each point.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
            " |          The query point or points.\n",
            " |          If not provided, neighbors of each indexed point are returned.\n",
            " |          In this case, the query point is not considered its own neighbor.\n",
            " |      \n",
            " |      n_neighbors : int, default=None\n",
            " |          Number of neighbors required for each sample. The default is the\n",
            " |          value passed to the constructor.\n",
            " |      \n",
            " |      return_distance : bool, default=True\n",
            " |          Whether or not to return the distances.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
            " |          Array representing the lengths to points, only present if\n",
            " |          return_distance=True.\n",
            " |      \n",
            " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
            " |          Indices of the nearest points in the population matrix.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      In the following example, we construct a NearestNeighbors\n",
            " |      class from an array representing our data set and ask who's\n",
            " |      the closest point to [1,1,1]\n",
            " |      \n",
            " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
            " |      >>> from sklearn.neighbors import NearestNeighbors\n",
            " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
            " |      >>> neigh.fit(samples)\n",
            " |      NearestNeighbors(n_neighbors=1)\n",
            " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
            " |      (array([[0.5]]), array([[2]]))\n",
            " |      \n",
            " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
            " |      element is at distance 0.5 and is the third element of samples\n",
            " |      (indexes start at 0). You can also query for multiple points:\n",
            " |      \n",
            " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
            " |      >>> neigh.kneighbors(X, return_distance=False)\n",
            " |      array([[1],\n",
            " |             [2]]...)\n",
            " |  \n",
            " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
            " |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
            " |          The query point or points.\n",
            " |          If not provided, neighbors of each indexed point are returned.\n",
            " |          In this case, the query point is not considered its own neighbor.\n",
            " |          For ``metric='precomputed'`` the shape should be\n",
            " |          (n_queries, n_indexed). Otherwise the shape should be\n",
            " |          (n_queries, n_features).\n",
            " |      \n",
            " |      n_neighbors : int, default=None\n",
            " |          Number of neighbors for each sample. The default is the value\n",
            " |          passed to the constructor.\n",
            " |      \n",
            " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
            " |          Type of returned matrix: 'connectivity' will return the\n",
            " |          connectivity matrix with ones and zeros, in 'distance' the\n",
            " |          edges are distances between points, type of distance\n",
            " |          depends on the selected metric parameter in\n",
            " |          NearestNeighbors class.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
            " |          `n_samples_fit` is the number of samples in the fitted data.\n",
            " |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
            " |          The matrix is of CSR format.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
            " |          of Neighbors for points in X.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> X = [[0], [3], [1]]\n",
            " |      >>> from sklearn.neighbors import NearestNeighbors\n",
            " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
            " |      >>> neigh.fit(X)\n",
            " |      NearestNeighbors(n_neighbors=2)\n",
            " |      >>> A = neigh.kneighbors_graph(X)\n",
            " |      >>> A.toarray()\n",
            " |      array([[1., 0., 1.],\n",
            " |             [0., 1., 1.],\n",
            " |             [1., 0., 1.]])\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the coefficient of determination of the prediction.\n",
            " |      \n",
            " |      The coefficient of determination :math:`R^2` is defined as\n",
            " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
            " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
            " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
            " |      The best possible score is 1.0 and it can be negative (because the\n",
            " |      model can be arbitrarily worse). A constant model that always predicts\n",
            " |      the expected value of `y`, disregarding the input features, would get\n",
            " |      a :math:`R^2` score of 0.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples. For some estimators this may be a precomputed\n",
            " |          kernel matrix or a list of generic objects instead with shape\n",
            " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
            " |          is the number of samples used in the fitting for the estimator.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True values for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
            " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
            " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
            " |      This influences the ``score`` method of all the multioutput\n",
            " |      regressors (except for\n",
            " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import neighbors\n",
        "help(neighbors.KNeighborsRegressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KnzVsnefEh4"
      },
      "source": [
        "As you can see in the help window, in scikit-learn, an estimator for classification/regression is a Python object that implements the methods fit(X, y) and predict(T). The constructor of an estimator takes as arguments the parameters of the model (in our case the basic parameters are the number of neighbours and the distance metric). So the first step is to create a KNN instance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RnFLYebsfEh5"
      },
      "outputs": [],
      "source": [
        "knn = neighbors.KNeighborsRegressor(n_neighbors = 5, metric = 'euclidean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98x8uXcOfEh6"
      },
      "source": [
        "We call our estimator instance `knn`. It now must be fitted to the data, that is, it must learn from the data. This is done by passing our training set to the fit method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbsJfKqQfEh6"
      },
      "outputs": [],
      "source": [
        "knn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-S_kM6ifEh6"
      },
      "source": [
        "Now you can predict new values, in particular, we can ask to the estimator which is the age of the first example in our test dataset (remember: by doing this, we are now comparing the features of this example to the features of all training samples, then determining what the closest neighbors are, then averaging the age of those neighbors as predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_TebEnkJfEh7"
      },
      "outputs": [],
      "source": [
        "y_pred = knn.predict(X_test[0].reshape(1, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS5zGDBEfEh7"
      },
      "source": [
        "We can calculate the mean squared error of the prediction we get before (for the first example of the test dataset) by calculating it ourselves or by using the [mean_squared_error()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function of scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dMZHjt52fEh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b6faa3-afb6-4ea0-e614-8142286f3415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 [10.]\n",
            "9.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(y_test[0], y_pred)\n",
        "print(mean_squared_error(y_pred, y_test[0].reshape(1, -1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UhPfQZGfEh8"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Build a 3-nearest-neighbor regressor in a similar manner as before. Predict the age for all training samples and test samples and compute the mean squared error of both prediction sets. Which predictions are better?**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M17HY9osfEh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf76b519-b919-458f-fb01-978e8b28cc87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.6375\n",
            "7.040476190476191\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the knn object\n",
        "knn =\n",
        "# Fit\n",
        "\n",
        "# Predict train\n",
        "y_pred_train =\n",
        "# Predict test\n",
        "y_pred_test =\n",
        "# print MSE train\n",
        "print(mean_squared_error(y_pred_train, y_train))\n",
        "# print MSE test\n",
        "print(mean_squared_error(y_pred_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7cAMV8ofEh9"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Now do the same but iterate over possible values for 'k' as the number of nearest neighbors taken into account, where k ranges between 1 and 50 (step size equal to 3). Plot the resulting errors of the models as a function of 'k'. 'k' is what we call a hyperparameter: a type of parameter that is (most often) tuned by hand according to the prediction task. For some datasets/tasks we will for example need more or less neighbors (as predictions tasks have varying difficulty).**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Vq3BS2nLfEh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "a5fc7a65-c117-4c84-8024-bccf906d9a29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb2e8c94110>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# use lists to store the results obtained from every iteration\n",
        "results_train = []\n",
        "results_test = []\n",
        "# for i in range(...):\n",
        "    # instantiate the model\n",
        "\n",
        "    # Fit\n",
        "\n",
        "    # Predict train\n",
        "\n",
        "    # Predict test\n",
        "\n",
        "    # calculate and store the MSE train\n",
        "\n",
        "    # calculate and store MSE test\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, 50, 3), results_train) # training MSEs\n",
        "plt.plot(range(1, 50, 3), results_test) # test MSEs\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('$k$ nearest neighbors')\n",
        "plt.legend(['Train MSE', 'Test MSE'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}