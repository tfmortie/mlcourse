{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJJ_c765KfKe"
      },
      "source": [
        "# PC Lab 6: Regularization Methods \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQv_DtYEKfKl"
      },
      "source": [
        "We have already seen linear regression to tackle regression problems. With linear regression, we model a continous outcome as a linear function of the features:\n",
        "\n",
        "$$\\hat{y} = w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p} = \\sum\\limits_{i=0}^{p}w_{i}x_{i}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkM2_pFeKfKm"
      },
      "source": [
        "This works well when there are a lot of training observations and when the number of features (the dimensionality of the problem) is not too large. However, there are a couple of situations where ordinary linear regression might give problems:\n",
        "\n",
        "* When the number of features $p$ becomes large with respect to the number of observations $n$, the variance of the model weights estimated by linear regression increases, which might result in poor predictive performance. Futhermore, there is no longer an analytical solution provided by least squares when $p > n$.\n",
        "* It is possible that there are a lot of uninteresting or redundant features. If we want a sparse and interpretable model, we might want to do feature selection to reduce $p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XzmPe28KfKn"
      },
      "source": [
        "In this lab, we will cover two solutions to the problems above: subset selection and regularization methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ips5qzMzKfKo"
      },
      "source": [
        "## 1. Subset selection methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luHc0r4HKfKp"
      },
      "source": [
        "In subset selection, we only use a subset of the features that are available. The goal is to come up with a model that is sparse and that generalizes better to unseen data. There are two main strategies for subset selection: in *best subset selection*, we fit all the $p \\choose k$ models for each $k = 1, 2, ..., p$ and retain the best model for each $k$. Finally, we select the model that performs best on some measure that controls for overfitting:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an2AElBWKfKq"
      },
      "source": [
        "![bestsubset](https://raw.githubusercontent.com/gdewael/teaching/main/predmod/lab6/img/best_subset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuf8noSoKfKq"
      },
      "source": [
        "This becomes quickly unfeasible for large values of $p$. Therefore, an alternative approach is to perform *stepwise selection*, which explores a much smaller set of feature combinations. Stepwise selection can be performed either backward or forward. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31dfsSMzKfKr"
      },
      "source": [
        "![forward](https://raw.githubusercontent.com/gdewael/teaching/main/predmod/lab6/img/forward.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j47zElu7KfKs"
      },
      "source": [
        "Finally, it is important to account for the fact that the MSE (or, equivalently, the RSS) will always go down on the training data as we add more and more features. To select the best model out of several candidates, it is important to have an estimate of the test error of each model. This can be done indirectly by using a metric that penalizes for model complexity such as the AIC or the adjusted $R^2$. Another option is to use cross-validation to get an estimate of the test error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "homZZs_VKfKs"
      },
      "source": [
        "**Let's apply subset selection on two datasets.** The first dataset contains features of mixtures used to produce concrete. The goal is to predict the compressive strength of the concrete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN7chQL-KfKt"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/tfmortie/mlcourse/refs/heads/main/06_regularization/concreteComprStrength.txt\n",
        "!wget https://raw.githubusercontent.com/tfmortie/mlcourse/refs/heads/main/06_regularization/fc_data_new.csv\n",
        "!wget https://raw.githubusercontent.com/tfmortie/mlcourse/refs/heads/main/06_regularization/communities.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7pKomwTKfKu"
      },
      "outputs": [],
      "source": [
        "# Read in the data\n",
        "import pandas as pd\n",
        "\n",
        "concretedata = pd.read_table('concreteComprStrength.txt', delim_whitespace=True, header=0, index_col=None)\n",
        "# Extract features and label for later usage\n",
        "X = concretedata.drop(['compressiveStrength'], axis=1)\n",
        "y = concretedata['compressiveStrength']\n",
        "\n",
        "print(concretedata.shape)\n",
        "concretedata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_RJOzdwKfKu"
      },
      "source": [
        "Let's build up an intuition for the code we need for doing best subset selection with linear regression. We will implement Algorithm 6.1 as shown above. First we split the data and look at how we can generate all possible combinations of features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0IhV2jkKfKv"
      },
      "outputs": [],
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "list_of_features = [\"ft.1\", \"ft.2\", \"ft.3\"]\n",
        "for subset in chain.from_iterable(combinations(list_of_features, r) for r in range(1, len(list_of_features)+1)):\n",
        "    print(subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV19YjZlKfKv"
      },
      "source": [
        "Now we can use this code to iterate not through `list_of_features` but through the possible features. For every subset we fit a model and keep it in a dictionary that records the score for that specific subset of features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMnSHnB5KfKw"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from itertools import chain, combinations\n",
        "\n",
        "list_of_features = X.columns\n",
        "scoring_dict = {}\n",
        "\n",
        "for subset in chain.from_iterable(combinations(list_of_features, r) for r in range(1, len(list_of_features)+1)):\n",
        "    X_subset = X[list(subset)]\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_subset, y)\n",
        "    scoring_dict[subset] = model.score(X_subset, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzD5G7tKKfKw"
      },
      "source": [
        "Taking five random samples from this dict to get an idea what's in it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOzdQkFnKfKx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.sample(list(scoring_dict.items()), 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_p3sxFTKfKx"
      },
      "source": [
        "Getting the best model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rSEYSmCKfKx"
      },
      "outputs": [],
      "source": [
        "max(scoring_dict, key=scoring_dict.get), scoring_dict[max(scoring_dict, key=scoring_dict.get)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIcXQqKaKfKy"
      },
      "source": [
        "We see that the best model (R² score-wise) is the one where all features are present. However, this is estimated on training data, so it might very well be that this result is due to overfitting. To do this in a better way, we will use cross-validation, as seen in the previous practical. The following code block expands upon the previous code block with these things in mind: **This code may take a little while to run.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKySUOLVKfKy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, cross_validate\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "list_of_features = X.columns\n",
        "scoring_dict = {}\n",
        "\n",
        "#NEW:\n",
        "# Define splitter object outside loop to ensure same splits over loop\n",
        "splitter = KFold(n_splits=5, shuffle=True, random_state=None)\n",
        "####\n",
        "\n",
        "for subset in chain.from_iterable(combinations(list_of_features, r) for r in range(1, len(list_of_features)+1)):\n",
        "    X_subset = X[list(subset)]\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # NEW: instead of simply fitting the model on all train data,\n",
        "    # now we use cross validation to fit and evaluate model\n",
        "    # cross_validate() returns a dict with key 'test_score' containing\n",
        "    # a list of  CV performances of every split\n",
        "    scores = cross_validate(model, X_subset.values, y.values, cv=splitter)\n",
        "    scoring_dict[subset] = np.mean(scores['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWcSra0HKfKz"
      },
      "source": [
        "Let's visualize how the test performance changes in function of how many features we had in the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo5SB38DKfKz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter([len(k) for k in scoring_dict.keys()], list(scoring_dict.values()), alpha = 0.25)\n",
        "plt.xlabel('number of features')\n",
        "plt.ylabel('R² score')\n",
        "# taking the best model for every 'n' number of features and plotting them separately:\n",
        "best_score_n = [\n",
        "    max([v for k, v in scoring_dict.items() if len(k) == i])\n",
        "    for i in range(1, X.shape[1]+1)\n",
        "]\n",
        "plt.scatter(range(1, X.shape[1]+1), best_score_n)\n",
        "print('Best feature combination:', max(scoring_dict, key=scoring_dict.get))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydij2VyKKfK0"
      },
      "source": [
        "We see that one certain combination of 5 features already results in a performance on +- the same level as the one with all features. This finding begs the question: could we have found this combination more efficiently, without testing out all possible feature combinations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO-V_gdFKfK0"
      },
      "source": [
        "Now, we will introduce a second dataset. This time, the features are measurements from a flow cytometry experiment. The 'SC' features measure scatter, and say something about the morphologhy of the cells (FSC: forwad scatter, SSC: sideway scatter). The 'FL' features are fluorescence features from different parts of the spectrum. There are two possible bacterial species present in the dataset. The goal is to classify the correct species based on the measurements from the flow cytometer. Species number one corresponds to *Pseudomonas putida*, while species number 6 is *Brachybacterium faecium*. We will use logistic regression to do the classification.\n",
        "\n",
        "This dataset has 10 features instead of the 8 features encountered in the previous dataset. We could repeat the same *best subset selection* method, but this time we will implement *stepwise forward selection*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C3fnDroKfK0"
      },
      "outputs": [],
      "source": [
        "# Read in the data\n",
        "bacterialdata = pd.read_csv('fc_data_new.csv', index_col=0)\n",
        "bacterialdata = bacterialdata.drop(['Width', 'Time'], axis=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao9GPki-KfK0"
      },
      "outputs": [],
      "source": [
        "bacterialdata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEgmigKqKfK1"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 1.1</b>: **Complete the code below that implements forward stepwise selection.**\n",
        "</div>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmQcmatPKfK1"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = bacterialdata.drop(['species'], axis=1)\n",
        "y = bacterialdata['species']\n",
        "scoring_dict = {}\n",
        "splitter = KFold(n_splits=5, shuffle=True, random_state=None)\n",
        "\n",
        "\n",
        "features_already_in_model = set()\n",
        "features_not_in_model = set(X.columns)\n",
        "\n",
        "# Implement code here "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg6Uy6UZKfK2"
      },
      "outputs": [],
      "source": [
        "for k, v in scoring_dict.items():\n",
        "    print(k, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jIYSkbtKfK2"
      },
      "source": [
        "## 2. Regularization methods: ridge and lasso regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMiV1GtsKfK3"
      },
      "source": [
        "Ridge and lasso regression are regularization techniques commonly employed in regression analysis to address the issue of overfitting. Overfitting occurs when a model learns the training data too well, capturing noise and leading to poor generalization on unseen data. Ridge regression adds a regularization term to the cost function (i.e. MSE), which penalizes large coefficients, encouraging the model to prioritize a simpler, smoother solution:\n",
        "\n",
        "$$\\sum_{i=1}^{n}\\Big(y_{i}-\\sum_{j=0}^{p}w_{j}x_{ij}\\Big)^{2}\\quad\\text{subject to}\\quad\\sum_{j=1}^{p}w_{j}^{2}=s.$$\n",
        "\n",
        "Lasso regression, on the other hand, uses a similar regularization approach but penalizes the absolute values of the coefficients: \n",
        "\n",
        "$$\\sum_{i=1}^{n}\\Big(y_{i}-\\sum_{j=0}^{p}w_{j}x_{ij}\\Big)^{2}\\quad\\text{subject to}\\quad\\sum_{j=1}^{p}|w_{j}|=s.$$\n",
        "\n",
        "The key distinction is that lasso regression can also induce sparsity, effectively setting some coefficients to zero, leading to feature selection. By adding these regularization terms, ridge and lasso regression help prevent overfitting by constraining the model's complexity, making it more robust and improving its performance on new, unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7JN31TbKfK3"
      },
      "source": [
        "### Linear models for high dimensional data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Z0ejcmKfK3"
      },
      "source": [
        "We will apply ridge regression on the Communities and Crime Data Set. The dataset contains 123 population statistics on 1994 communities. We would like to predict the number of violent crimes per 100000 inhabitants. This is the final column of the dataframe. Of the 123 features, a lot contain missing values, so we will drop these columns and use only 99 features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqLsPmtzKfK3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('./communities.csv')\n",
        "\n",
        "# Drop columns with missing values\n",
        "data = data.dropna(axis=1)\n",
        "print(data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aMRVhH6KfK4"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 2.1</b>: **Use linear regression and ridge regression to predict the number of violent crimes per 100,000 inhabitants. Use 5-fold cross-validation to evaluate both models. The [_RidgeCV_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) class in Scikit-learn automatically performs cross-validation to tune the hyperparameter that determines the amount of regularization, so you don't need to implement a second cross-validation loop. Which model performs best?**\n",
        "</div>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSCLkUQ3KfK4"
      },
      "source": [
        "**Before we apply ridge regression, it's important to make sure that all the features are on the same scale. If one of the features is on a completely different scale (let's say, income can be measured in dollars or in thousands of dollars), this might lead the ridge regression coefficient to change substantially because of the penalty term in the optimization problem. We can make sure that all the features are on the same scale by means of standardization:**\n",
        "\n",
        "$$\\tilde{x}_{ij} = \\frac{x_{ij}-\\bar{x}_{j}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{ij} - \\bar{x}_{j})^2}}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgYKuuOhKfK4"
      },
      "source": [
        "We can do this with the [_StandardScaler_](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class in Scikit-learn. We will do the scaling each time in the cross-validation loop using only training data statistics (*do you still remember why we only use training statistics to do this?*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_n7gdMNKfK4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, RidgeCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AddUmtcFKfK5"
      },
      "outputs": [],
      "source": [
        "# Select X and y\n",
        "y = data['ViolentCrimesPerPop'].values\n",
        "X = data.drop(['ViolentCrimesPerPop'], axis=1).values\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "linreg_scores = []\n",
        "ridge_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # scale X\n",
        "\n",
        "\n",
        "    # initialize the models\n",
        "\n",
        "\n",
        "    # fit, and store the accuracy on the test set\n",
        "\n",
        "print('Average validation fold R² of linear regression: {}'.format(np.mean(linreg_scores)))\n",
        "print('Average validation fold R² of ridge regression: {}'.format(np.mean(ridge_scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC670WUaKfK5"
      },
      "source": [
        "Now suppose that we don't have 99 features, but four times as many features. And suppose that a lot of features are correlated. This situation is very common in lots of datasets. We will mimic this situation by adding correlated features to our original feature matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbNgSkx2KfK5"
      },
      "outputs": [],
      "source": [
        "X_1 = X + np.random.normal(loc=0, scale=0.05, size=(X.shape))\n",
        "X_2 = X + np.random.normal(loc=0, scale=0.1, size=(X.shape))\n",
        "X_3 = X + np.random.normal(loc=0, scale=0.01, size=(X.shape))\n",
        "X_expanded = np.concatenate((X, X_1, X_2, X_3), axis=1)\n",
        "\n",
        "X_expanded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Kjz4_rKfK5"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 2.2</b>: **Repeat the comparison between ridge regression and linear regression from above, but with the new feature matrix that contains correlated features. What happens with the performance of linear regression? What happens with the regularization parameter?**\n",
        "</div>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09hjGXMiKfK5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWgjkB4AKfK6"
      },
      "source": [
        "### Lasso regression and feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_82GZJCKfK6"
      },
      "source": [
        "Suppose that we are interested in selecting only a couple of features out of a high dimensional dataset. Let's fit ridge regression and lasso regression on the data and look at the model coefficients for both models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4szzL8tKfK6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ridgemodel = RidgeCV(cv=5)\n",
        "lassomodel = LassoCV(cv=5, max_iter=10000)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Scale X\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "ridgemodel.fit(X_train, y_train)\n",
        "ridgescore = np.round(ridgemodel.score(X_test, y_test),2)\n",
        "lassomodel.fit(X_train, y_train)\n",
        "lassoscore = np.round(lassomodel.score(X_test, y_test),2)\n",
        "\n",
        "# Plot of the coefficients for ridge regression\n",
        "fig, ((ax1, ax2)) = plt.subplots(figsize=(15, 10), nrows=2)\n",
        "pd.Series(ridgemodel.coef_).plot(kind='bar', ax=ax1)\n",
        "ax1.set_title('Ridge regression coefficients. Test set R²: {}'.format(ridgescore)).set_fontsize(40)\n",
        "ax1.get_xaxis().set_ticks([])\n",
        "ax1.set_xlabel('Features')\n",
        "\n",
        "# Plot of the coefficients for the lasso\n",
        "pd.Series(lassomodel.coef_).plot(kind='bar', ax=ax2)\n",
        "ax2.set_title('Lasso regression coefficients. Test set R²: {}'.format(lassoscore)).set_fontsize(40)\n",
        "ax2.get_xaxis().set_ticks([])\n",
        "ax2.set_xlabel('Features');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlfJcYzJKfK6"
      },
      "source": [
        "The lasso applies regularization by constraining the sum of the absolute values of the model coefficients (the L1-norm). A result of this is that a lot of model coefficients are set to zero: the lasso performs **feature selection**. This is not the case for ridge regression: the model weights are rarely set to zero. Feature selection is a nice property if we want an interpretable model. Let's list the features with non-zero coefficients in the lasso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wVKQdtUKfK7"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,9))\n",
        "sns.barplot(x= pd.Series(lassomodel.coef_[lassomodel.coef_ != 0]),\n",
        "            y=data.drop(['ViolentCrimesPerPop'], axis=1).columns[lassomodel.coef_ != 0],\n",
        "            palette=['steelblue' if n > 0 else 'coral' for n in lassomodel.coef_[lassomodel.coef_ != 0]],\n",
        "            ax=ax);\n",
        "ax.set_title('Lasso model coefficients for the Community Crime dataset').set_fontsize(20);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "i8lNaR5LKfK2",
        "zMwkhvMrKfK7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
