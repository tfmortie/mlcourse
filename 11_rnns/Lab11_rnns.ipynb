{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrQv9yrEbWo3"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** are a type of neural network designed for processing sequential data, such as time series, text, or any data with temporal dependencies. Unlike traditional neural networks, RNNs have loops that allow information to persist, making them well-suited to sequences where previous information is relevant to understanding the current state.\n",
    "\n",
    "In an RNN, each element in a sequence is processed one at a time, with each step’s output influenced by both the input at that time step and the hidden state from the previous step. This design enables RNNs to maintain a form of memory, capturing dependencies across time. \n",
    "\n",
    "RNNs are widely used in tasks such as language modeling, speech recognition, and predictive analytics, where understanding past information is essential to predicting future events. In general, RNNs can be used in several settings, as pictured below:\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width = 500>\n",
    "\n",
    "Examples of the settings in the picture:\n",
    "\n",
    "- _One to one_: vanilla MLPs that map a fixed-size vector to another vector for classification or regression.\n",
    "- _One to many_: Image captioning, given an input embedding (obtained with a CNN), a textual caption of variable length is generated.\n",
    "- _Many to one_: (1) Sentence classification such as sentiment analysis or (2) image generation from text. In both cases variable-length input texts are given as input and a fixed-size vector is generated.\n",
    "- _Many to many_: (1) Machine translation of a variable-length sentence to another variable-length sentence or (2) transcription of a variable-length .mp3 audio to a variable-length text.\n",
    "- _Many to many (1-to-1 correspondence)_: (1) Video classification where one label for a variable number of frames in the video is predicted (the video frame embedding can be obtained with a CNN and then fed into a RNN), (2) autoregressive language modeling where the next word in the sentence is predicted for generative purposes or (3) word classification where every word is classified as belonging to a category.\n",
    "\n",
    "Note that these settings are not exclusive to RNNs. In fact, any network type that works on variable-length input sequences can be used towards these ends. Most famously of which are of course, **Transformers**, which have all but replaced RNNs in NLP and many other fields. An explanation and implementation of Transformers is out of the scope for this course. It suffices to know that RNNs process input sequence sequentially through memory cells, whereas Transformers do it in parallel through an $N \\times N$ attention matrix, with $N$ the number of input tokens/vectors. Other than RNNs and Transformers, convnets can also be used on variable-length inputs: a 1D kernel can equally well convolve over a sequence of length $100$ as $1000$. It is only because of the fully-connected layers at the end of the network that typical CNNs are applicable on fixed-size inputs only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFhcwjLBbWo9"
   },
   "source": [
    "## Part 1: Autoregressive modelling\n",
    "\n",
    "Autoregressive modelling is the task of trying to predict the next token in a sequence, given previous tokens. In this PC-lab we will explore autoregressive modelling techniques on a particular dataset from the city of Ghent, called *Fietstelpaal Coupure-Links 2023 Gent*.\n",
    "\n",
    "<img src=\"https://images0.persgroep.net/rcs/4cQwm-ofvb3eyIKMWnNf5axxLHg/diocontent/217261403/_fitwidth/694/?appId=21791a8992982cd8da851550a453bd7f&quality=0.8\" width = 500>\n",
    "\n",
    "The processed dataset contains data from the [Coupure-Links bicycle counter](https://data.stad.gent/explore/dataset/fietstelpalen-gent/table/) for the year 2023 and consists of the following variables:\n",
    "\n",
    "* Date_hour: date and hour encoded in the following format: YYYY-MM-DDTHH.\n",
    "* Totaal: the total number of cyclists that passed the counter.\n",
    "\n",
    "Let's start with the exploration of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ArqjiofwdVJx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_hour</th>\n",
       "      <th>Totaal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01T00</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01T01</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01T02</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01T03</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01T04</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>2023-06-30T19</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>2023-06-30T20</td>\n",
       "      <td>325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>2023-06-30T21</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4341</th>\n",
       "      <td>2023-06-30T22</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>2023-06-30T23</td>\n",
       "      <td>208.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4343 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date_hour  Totaal\n",
       "0     2023-01-01T00   111.0\n",
       "1     2023-01-01T01   201.0\n",
       "2     2023-01-01T02   170.0\n",
       "3     2023-01-01T03   144.0\n",
       "4     2023-01-01T04   155.0\n",
       "...             ...     ...\n",
       "4338  2023-06-30T19   465.0\n",
       "4339  2023-06-30T20   325.0\n",
       "4340  2023-06-30T21   260.0\n",
       "4341  2023-06-30T22   239.0\n",
       "4342  2023-06-30T23   208.0\n",
       "\n",
       "[4343 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/tfmortie/mlcourse/main/11_rnns/train_data.csv\", \"./train_data.csv\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/tfmortie/mlcourse/main/11_rnns/test_data.csv\", \"./test_data.csv\")\n",
    "\n",
    "train_data = pd.read_csv(\"train_data.csv\", sep = \",\")\n",
    "test_data = pd.read_csv(\"test_data.csv\", sep = \",\")\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCTYwonrNbEd"
   },
   "source": [
    "Let's encode the hour as separate feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "nqtJ0rjyLZuw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_hour</th>\n",
       "      <th>Totaal</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01T00</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01T01</td>\n",
       "      <td>201.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01T02</td>\n",
       "      <td>170.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01T03</td>\n",
       "      <td>144.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01T04</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date_hour  Totaal  Hour\n",
       "0  2023-01-01T00   111.0   0.0\n",
       "1  2023-01-01T01   201.0   1.0\n",
       "2  2023-01-01T02   170.0   2.0\n",
       "3  2023-01-01T03   144.0   3.0\n",
       "4  2023-01-01T04   155.0   4.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Hour\"] = train_data[\"Date_hour\"].str.split(\"T\", expand = True)[1].astype(float)\n",
    "test_data[\"Hour\"] = test_data[\"Date_hour\"].str.split(\"T\", expand = True)[1].astype(float)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45I4eXgtdUrm"
   },
   "source": [
    "Architecturally, autoregressive modelling of a timeseries looks like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tfmortie/mlcourse/main/11_rnns/AR.png\" width = 500>\n",
    "\n",
    "For every point in the timeseries, the *input token* consists of the value at that timepoint and, optionally, extra covariates pertaining to that timepoint. Conceptually, this is similar to the channels in a CNN, as there we also had input tokens (e.g. pixels) with multiple values (e.g. the 3 RGB values) per token. In our case, extra covariates could be for example the hour at which that timepoint was taken. This input sequence will go into the RNN, which will keep a hidden layer which acts as a memory bank. The memory bank of every input will consist of a combination of the information at that time point and the information coming in from the memory cell at the previous time point. The specific way this information is brought together depends on the specific construction of the RNN. We refer you to the theory lectures for details. The most popular constructions are the LSTM and the GRU memory cells. For every timestep, the model outputs a vector which (for this purpose) needs to be linearly recombined to one number as our goal is to predict the value of the next timestep (i.e. a regression task).\n",
    "\n",
    "Code-wise, it is important to know that for a given sequence, we have an input $x$ consisting of the timepoints in that sequence, and an output $y$, consisting of the same points, but **shifted one time-step to the left**. **Because of the directionality of the RNN, for every time-step, it will predict the next point given only the preceding ones.**\n",
    "\n",
    "In this PC-lab, we will use the hour of the timepoint as covariate.\n",
    "\n",
    "For training, we can't put all days in to our model as one sample. Just like the reason for doing batches in other networks is that: it is more computationally efficient, and it allows us to have training steps on different parts of data with some stochasticity to it, allowing us to jump out of local minima.\n",
    "\n",
    "**For RNNs, another reason is that our \"actual\" neural network depth is essentially decided by our input length**, so if we send in a sample containing a thousand input tokens, we also backpropagate through a thousand layers, and our computers will surely crash. In addition, it is not reasonable to assume the number of cyclists hundreds of days past still influences the number of cyclists now. So, the problem of batching our sequence becomes one of weighing two factors: how long of a sequence can our model handle, and how much context (in number of preceding tokens) do our models need for prediction?\n",
    "\n",
    "Here we will take a batch size of 48 as a default, meaning that our samples will always coincide with two consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "H_xO94-nfEUJ"
   },
   "outputs": [],
   "source": [
    "def generate_batches(sequence, seqlen = 48):\n",
    "    batches = []\n",
    "    for i in np.arange(0, len(sequence) - seqlen, seqlen):\n",
    "        batches.append(sequence[i:i+seqlen])\n",
    "    return torch.stack(batches)\n",
    "\n",
    "train_batches = generate_batches(torch.tensor(train_data[[\"Hour\", \"Totaal\"]].values.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DeB-YAajJye"
   },
   "source": [
    "Let's see how a sample looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "oV9G5RHzmiW1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 48, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "kqNij14zjIlf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0., 111.],\n",
       "        [  1., 201.],\n",
       "        [  2., 170.],\n",
       "        [  3., 144.],\n",
       "        [  4., 155.],\n",
       "        [  5., 140.],\n",
       "        [  6., 109.],\n",
       "        [  7.,  75.],\n",
       "        [  8.,  43.],\n",
       "        [  9.,  42.],\n",
       "        [ 10.,  50.],\n",
       "        [ 11.,  76.],\n",
       "        [ 12.,  94.],\n",
       "        [ 13., 113.],\n",
       "        [ 14., 125.],\n",
       "        [ 15., 173.],\n",
       "        [ 16., 109.],\n",
       "        [ 17., 100.],\n",
       "        [ 18., 116.],\n",
       "        [ 19.,  67.],\n",
       "        [ 20., 120.],\n",
       "        [ 21.,  82.],\n",
       "        [ 22.,  60.],\n",
       "        [ 23.,  29.],\n",
       "        [  0.,  11.],\n",
       "        [  1.,  10.],\n",
       "        [  2.,   2.],\n",
       "        [  3.,   5.],\n",
       "        [  4.,  15.],\n",
       "        [  5.,  33.],\n",
       "        [  6.,  58.],\n",
       "        [  7., 169.],\n",
       "        [  8., 257.],\n",
       "        [  9., 198.],\n",
       "        [ 10., 129.],\n",
       "        [ 11., 123.],\n",
       "        [ 12., 206.],\n",
       "        [ 13., 200.],\n",
       "        [ 14., 192.],\n",
       "        [ 15., 208.],\n",
       "        [ 16., 266.],\n",
       "        [ 17., 363.],\n",
       "        [ 18., 299.],\n",
       "        [ 19., 177.],\n",
       "        [ 20.,  74.],\n",
       "        [ 21.,   8.],\n",
       "        [ 22.,   7.],\n",
       "        [ 23.,  21.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fogv0s--jiXD"
   },
   "source": [
    "We could give our input to the model like this, as the hour is a numerical value which can be interpreted using linear layers. It would make more sense to treat the hour variable as categorical and encode it using dummy variables (one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "1XAhDa7pmdNd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 48, 24])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_hour = torch.nn.functional.one_hot(train_batches[:, :, 0].long())\n",
    "\n",
    "one_hot_hour.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1RTNwK3Amv21"
   },
   "outputs": [],
   "source": [
    "train_batches = torch.cat([train_batches[:, :, [1]], one_hot_hour], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "5h43ZQWKm3G4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 48, 25])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Aw3OsNCom4Ic"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[111.,   1.,   0.,  ...,   0.,   0.,   0.],\n",
       "        [201.,   0.,   1.,  ...,   0.,   0.,   0.],\n",
       "        [170.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  8.,   0.,   0.,  ...,   1.,   0.,   0.],\n",
       "        [  7.,   0.,   0.,  ...,   0.,   1.,   0.],\n",
       "        [ 21.,   0.,   0.,  ...,   0.,   0.,   1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kHh7PNFONWN"
   },
   "source": [
    "Let's min-max scale the outputs. Remember: if you don't do this, your loss function will be of a very big scale, affecting learning (i.e. you will need lower learning rates).\n",
    "Hence, scaling allows us to make a better guess as to what a good learning rate will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "D0fRtX3GOjFQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0583, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1056, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0893, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0042, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "         [0.0037, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "        [[0.0063, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0016, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0688, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "         [0.0678, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.0494, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "        [[0.0163, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0095, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0736, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "         [0.0657, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.0641, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0583, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0305, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0200, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1272, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.0809, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000],\n",
       "         [0.0347, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0194, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0079, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0063, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1760, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.0914, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000],\n",
       "         [0.0573, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0315, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0242, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0147, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1745, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.1088, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000],\n",
       "         [0.0730, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_cyclists_train = train_batches[:, :, 0].max()\n",
    "min_cyclists_train = train_batches[:, :, 0].min()\n",
    "train_batches[:, :, 0] = (train_batches[:, :, 0]-min_cyclists_train)/(max_cyclists_train-min_cyclists_train)\n",
    "train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lLoumfYPSVE"
   },
   "source": [
    "Finally, let's prepare some batches in a similar way for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "s7UOVAf7PUX0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0788, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0483, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0257, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0904, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.0788, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.0504, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches = generate_batches(torch.tensor(test_data[[\"Hour\", \"Totaal\"]].values.astype(np.float32)))\n",
    "one_hot_hour = torch.nn.functional.one_hot(test_batches[:, :, 0].long())\n",
    "test_batches = torch.cat([test_batches[:, :, [1]], one_hot_hour], axis = 2)\n",
    "test_batches[:, :, 0] = (test_batches[: ,: ,0]-min_cyclists_train)/(max_cyclists_train-min_cyclists_train)\n",
    "test_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autQkT0vn_L5"
   },
   "source": [
    "### RNNs in PyTorch\n",
    "\n",
    "In this PC-lab we will use the GRU, but note that other RNN-types work similarly in PyTorch.\n",
    "\n",
    "[Documentation for the GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
    "\n",
    "_Note_: The weird shape expectations (such as not expecting batches to come first by default) are a consequence of optimizations that PyTorch has implemented so the RNNs run efficiently on data with variable input sequence lengths (such as sentences). For this PC lab, we have batched our sequences so that they have constant sequence length, so we can add the argument `batch_first = True`.\n",
    "\n",
    "Let's create a GRU and some toy data to see how it all works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "m_Z39C9soQXX"
   },
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size = 64, hidden_size = 512, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "6XU72dghoRnu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 50, 512]), torch.Size([1, 2, 512]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 50, 64)\n",
    "output, h_n = gru(x)\n",
    "output.shape, h_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7h1DZBHoX9o"
   },
   "source": [
    "Explanation of the outputs: `h_n` is the hidden representation of the last hidden memory cell. It can be seen as a summarized representation of the content of the whole input (if one wants a single output for a whole sequence as in e.g. sentence classification). `output` will return the output representation of the RNN for every input token: look back at the picture in the introduction of this part of the PC lab for more intuition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jPd_xQpolKr"
   },
   "source": [
    "For autoregressive modeling of time series, we should have an output for every input, mainly: the prediction of the next timepoint. For our purpose, we are interested to predict a single output per timepoint, whereas we can see that this is not the case for our GRU model as it is now. We can remedy this with a simple linear layer\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE:</b>\n",
    "<p> Implement an autoregressive GRU for cycler forecasting by completing the code below. The model should contain a recurrent layer, and a layer that takes the outputs of the GRU at each timestep and manipulates their dimensions so that the output dimensionality of each token is equal to one. Keep in mind how many input variables we have in our dataset.\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "Test your model with some toy data down below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13mJg1lqomei"
   },
   "outputs": [],
   "source": [
    "class CyclerForecaster(nn.Module):\n",
    "    # def __init__() ....\n",
    "\n",
    "    # def forward() ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPgN3P4NpvJL"
   },
   "outputs": [],
   "source": [
    "model = CyclerForecaster()\n",
    "\n",
    "x = torch.randn(2, 48, 25)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwqJ1fTpnCWa"
   },
   "source": [
    "To create an input and an output, we have to do the above-mentioned shifting. In practice, this means that we take all but the last timepoints to create X, and all but the first timepoints to create Y, hence creating shifted X,Y pairs.\n",
    "\n",
    "Additionally, for Y, we only want to keep the first variable, meaning the number of cyclists itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMw9_32bf9hj"
   },
   "outputs": [],
   "source": [
    "X_train = train_batches[:, :-1]\n",
    "\n",
    "y_train = train_batches[:, 1:]\n",
    "y_train = y_train[:, :, [0]]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhHNqiqHQTiM"
   },
   "source": [
    "Doing the same for the test data, and putting the things into datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvr5sBhDhR5G"
   },
   "outputs": [],
   "source": [
    "X_test = test_batches[:, :-1]\n",
    "y_test = test_batches[:, 1:, [0]]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM5AD2u7rNA2"
   },
   "source": [
    "Note that we are just taking the first 80% of the data as training and the last 20% as validation set. Normally, we shuffle our data so that we are not biased. In this case, however, we can make a case for doing it our way: because the samples are ordered by day and month, the last samples will be from the summer months, where we may expect different patterns (June: exams, July: vacations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmpFl85TvvF0"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE:</b>\n",
    "<p> Implement the training loop for the Cycler Forecaster using the same principles from last PC labs. Keep in mind that unlike previous PC labs, we are now training a regression task. You will also need to increase the number of epochs as our dataset is quite small and each epoch only constitutes a small number of training steps.\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RclHsVTChfYQ"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XntUGcU0wR8m"
   },
   "source": [
    "To evaluate our model beyond looking at a loss function going down: we can look at the autoregressive results for a random test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYeAfeVhhnCA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(y_test[0])), y_test[0])\n",
    "plt.plot(np.arange(len(y_test[0])), model(X_test[[0]]).detach().squeeze(0).numpy())\n",
    "plt.legend([\"True\", \"Predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtf9XU0BSh9M"
   },
   "source": [
    "You should see that at the beginning of the second day, the model overshoots its prediction by a wide margin. This is because most probably the next day is a weekend day. After that, for the prediction of the next step, we give it the true input of that (previously-wrongly predicted) timestep, and because it recognizes that by this low value it should be a weekend day, the subsequent predictions are also low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXMxdryOwuY5"
   },
   "source": [
    "\n",
    "\n",
    "This is not really forecasting though: at every timestep, we are predicting only one timestep (hour) in advance. To really do forecasting for a longer time limit, we should feed the predictions of the model back into the model, like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tfmortie/mlcourse/main/11_rnns/generation.png\" width = 500>\n",
    "\n",
    "To perform this, we will create a helper function that extracts the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiyoKVzAx_sc"
   },
   "outputs": [],
   "source": [
    "def generate_next_timestep(previous, model):\n",
    "    with torch.no_grad():\n",
    "        output = model(previous.unsqueeze(0))[0, -1]\n",
    "\n",
    "    return output\n",
    "\n",
    "def generate_n_timesteps(previous, model, n = 5):\n",
    "    for _ in range(n):\n",
    "        prediction = generate_next_timestep(previous, model)\n",
    "        # to make a next prediction, we not only need to add the prediction but also the covariates (hour) to our next input:\n",
    "        next_timestep_hour = nn.functional.one_hot((previous[-1, 1:].argmax() + 1) % 24, num_classes = 24)\n",
    "        new_input = torch.cat([prediction, next_timestep_hour])\n",
    "        previous = torch.cat([previous, new_input.unsqueeze(0)])\n",
    "\n",
    "    return previous[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROyU81wA8Ixe"
   },
   "outputs": [],
   "source": [
    "sample_index = 0 # any index in the test set\n",
    "priming_points = ... # how many observed timepoints to give the model to start predicting from\n",
    "generate_steps = ... # how many timepoints in the future the model should forecast\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(X_test[sample_index])), X_test[sample_index, :, 0])\n",
    "predictions = generate_n_timesteps(X_test[sample_index, :priming_points], model, n = generate_steps).numpy()\n",
    "plt.plot(np.arange(len(predictions)), predictions)\n",
    "plt.axvline(x = priming_points-1, color = \"green\", linestyle = \":\")\n",
    "plt.legend([\"True\", \"Predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZytOD_XA0tx"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>THOUGHT EXERCISE:</b>\n",
    "<p> Play around with the code for forecasting bicyclists by changing up which sample you forecast (\"sample_index\"), how much initial input points you give the model (\"priming points\") and how far in time it should generate after that (\"generate steps\"). Can you find obvious failure states? Can you think of ways how to remedy these failure modes?\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2N3F1zyBpQg"
   },
   "source": [
    "## Part 2: time series classification\n",
    "\n",
    "In the next part, we will explore a many-to-one scenario. To keep things simple, we will work on the same dataset and try to predict from the series of a single day whether that day was a weekend or a weekday.\n",
    "\n",
    "To do this, we first need to know which days were weekdays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itJrohODCD5a"
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7msK_1LRVLM"
   },
   "source": [
    "Following code adds an indicator variable to the data that signifies if the date is a weekday or not. We'll use this as \"y\" values to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3ZcSVjICFRo"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def isweekday(date):\n",
    "    return datetime.fromisoformat(date).weekday() < 5\n",
    "\n",
    "train_data[\"Date\"] = train_data[\"Date_hour\"].str.split(\"T\", expand = True)[0]\n",
    "test_data[\"Date\"] = test_data[\"Date_hour\"].str.split(\"T\", expand = True)[0]\n",
    "\n",
    "train_data[\"isweekday\"] = [int(isweekday(i)) for i in train_data[\"Date\"]]\n",
    "test_data[\"isweekday\"] = [int(isweekday(i)) for i in test_data[\"Date\"]]\n",
    "train_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQKStrAtHLVs"
   },
   "source": [
    "With this data added, we can process our data into X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAhtjoCMDdlM"
   },
   "outputs": [],
   "source": [
    "train_batches = generate_batches(torch.tensor(train_data[[\"isweekday\", \"Hour\", \"Totaal\"]].values.astype(np.float32)), seqlen = 24)\n",
    "one_hot_hour = torch.nn.functional.one_hot(train_batches[:, :, 1].long())\n",
    "train_batches = torch.cat([train_batches[:, :, [0, 2]], one_hot_hour], axis = 2)\n",
    "train_batches[:, :, 1] = train_batches[:, :, 1] / max_cyclists_train\n",
    "\n",
    "y_train = train_batches[:, 0, 0]\n",
    "X_train = train_batches[:, :, 1:]\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvtDFbMhDh4U"
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygrXoqTTYBO1"
   },
   "source": [
    "The same for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORpj437EYCvk"
   },
   "outputs": [],
   "source": [
    "test_batches = generate_batches(torch.tensor(test_data[[\"isweekday\", \"Hour\", \"Totaal\"]].values.astype(np.float32)), seqlen = 24)\n",
    "one_hot_hour = torch.nn.functional.one_hot(test_batches[:, :, 1].long())\n",
    "test_batches = torch.cat([test_batches[:, :, [0, 2]], one_hot_hour], axis = 2)\n",
    "test_batches[:, :, 1] = test_batches[:, :, 1] / max_cyclists_train\n",
    "\n",
    "y_test = test_batches[:, 0, 0]\n",
    "X_test = test_batches[:, :, 1:]\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBAGqj4ReEj"
   },
   "source": [
    "Examine if the shapes are conforming to your expectations as to what we expect it to be for timeseries classification. What does every dimension in X and y signify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DMgGIEGGc29"
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L15DJ0eeHHpL"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE:</b>\n",
    "<p> Implement a time series classification model by copying the code above and adapting it.\n",
    "\n",
    "Steps you need to take:\n",
    "- Change the RNN model so that it is adapted for many to one tasks\n",
    "- Change the loss function for binary classification\n",
    "- Implement a way to keep track of accuracies instead of only losses\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "You should be able to obtain an accuracy of at least 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBQaLX3xHvQ6"
   },
   "outputs": [],
   "source": [
    "class TimeseriesClassifier(nn.Module):\n",
    "    # def __init__() ....\n",
    "\n",
    "    # def forward() ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOhRw5MBIscM"
   },
   "outputs": [],
   "source": [
    "# TRAINING CODE GOES HERE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_OAU_xKaRYy"
   },
   "source": [
    "Let's plot all the test data and see what the model still got wrong. In the following code. Green timeseries are weekdays and blue timeseries are weekends. The transparent ones are correctly predicted, whereas the dotted ones are predicted wrongly. (So a blue dotted line means a weekend predicted as being a weekday and vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OwVAqSuajPf"
   },
   "outputs": [],
   "source": [
    "preds = (model(X_test) > 0).detach().int().numpy()\n",
    "trues = y_test.numpy()\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    plt.plot(np.arange(24), X_test[i, :, 0].numpy(),\n",
    "             color = [\"blue\", \"green\"][int(trues[i])],\n",
    "             ls = [\":\", \"-\"][int(trues[i] == preds[i])],\n",
    "             alpha = [1, 0.1][int(trues[i] == preds[i])],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQXDAGoUZGXh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
