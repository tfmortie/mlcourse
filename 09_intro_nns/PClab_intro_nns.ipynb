{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjxvlCzY6h4t"
      },
      "source": [
        "<img src='https://analyticsindiamag.com/wp-content/uploads/2022/04/Screenshot-2022-04-06-at-9.55.11-PM.png' width=750>\n",
        "\n",
        "*Images generated by a deep neural network that interprets text to generate images (DALL·E 2)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xqzgGkp6Z0a"
      },
      "source": [
        "# PC lab: intro to Neural networks & PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNEdF_h4-AmQ"
      },
      "source": [
        "Deep learning is the subfield of machine learning that concerns neural networks with representation learning capabilities. As of recent years, it is arguably the most quickly growing field within machine learning, enjoying major breakthroughs every year (Listing a couple ones from last year(s): ChatGPT, AlphaFold v2, DALL·E 2, AlphaZero). Although the popularity of neural nets is a recent phenomenon, they were first described by Warren McCulloch and Walter Pitts in 1943. Early progress in training competitive neural networks was stalled by a multitude of reasons, such as the limited computer resources, sub-optimal network architectures and the use of smaller datasets. In this PC-lab we will introduce you to the basics of implementing a neural network using contemporary practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jqUbY3CO_fY"
      },
      "source": [
        "## 1 Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19SzILbZQkIL"
      },
      "source": [
        "### From linear models to neurons to neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CX3qibbPBs7"
      },
      "source": [
        "The core unit of every (artificial) neural network is considered the neuron. Every neuron can be observed as a linear combination of **one or more inputs** $\\mathbf{a}$ with weights $\\mathbf{w}$ (and optionally adding a **bias** $b$), outputting a **single output** $z$:\n",
        "\n",
        "$z = \\sum\\limits_{i=1}^{n}(a_iw_i) + b$\n",
        "\n",
        "equivalently written as dot product:\n",
        "\n",
        "$z = \\mathbf{a} \\cdot \\mathbf{w} + b$\n",
        "\n",
        "When multiple neurons are placed next to eachother, we get multiple output neurons, this is called a **layer**. Mathematically, our weights and biases (intercepts) now become a matrix and vector respectively, and we obtain as output a vector $\\mathbf{z}$:\n",
        "\n",
        "$\\mathbf{z} = \\mathbf{a} \\cdot \\mathbf{W} + \\mathbf{b}$\n",
        "\n",
        "Multiple **layers** can be stacked sequentially to make up a deep neural network. Performing this gives us a big linear model, because multiple matrix multiplications performed consecutively can also be written as just one: (i.e. $\\mathbf{x} \\cdot \\mathbf{W_1} \\cdot \\mathbf{W_2} ...$ could also be written as just $\\mathbf{x} \\cdot \\mathbf{W_{all}}$). To obtain the *deep learning magic*, we need to make the whole thing **non-linear**. We do this by adding **activation functions** after every (hidden) layer. The most classical activation is the sigmoid activation $\\sigma()$, used also in logistic regression. Nowadays, we usually opt for a more simple activation function: the **ReLU** $\\texttt{ReLU}(z) = max(0,z)$. This function has the favorable property that its derivative is very efficient to compute (1 when $z$ is positive, 0 when it is negative). It also acts as a switch: a neuron will have a \"dead\" ($0$) activation whenever $z$ is negative.\n",
        "\n",
        "For the output layer of a neural network: our activation depends on the task at hand. For binary classification, we use sigmoid to constrain our output between 0 and 1. For multi-class, we use a softmax operation so the output of all neurons sums to 1. For regression, we simply do not use an activation (or a custom one depending on your data: if you already know that your outputs can't be negative but can take all positive numbers ($\\mathbb{R}^+$), then maybe a ReLU activation in the output nodes makes sense)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o7g3tpiMXW4"
      },
      "source": [
        "In order to build more intuition for neural networks: consider the following figure where we \"visually build up\" a neural network starting from Linear regression with four input features (**a**), to Logistic regression (**b**), to an archetypical output neuron with ReLU activation (**c**). For multi-output settings, we visualize multi-output regression (**d**), multi-label classification (more than one class can be 1 in a sample) (**e**), and multi-class classification via softmax (**f**). Finally, a simple neural network with two hidden layers for binary classification (sigmoid output head) is shown under **g**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/BioML-UGent/MLLS/main/11_intro_nns/lr2nn.png'>\n",
        "\n",
        "**This figure makes it crystal clear that the most simple neural network is just a bunch of linear regressions stacked on top of eachother with non-linearities inbetween.** More advanced neural network architectures exist that modify how we make information flow between inputs. In this example, everything is just connected to everything with linear weights. This type of neural network is what we call an **MLP** or a **multi layer perceptron**.\n",
        "\n",
        "Depending on the type of output, different loss functions may be used:\n",
        "<img src='https://raw.githubusercontent.com/BioML-UGent/MLLS/main/11_intro_nns/lossfunctions.png' width=800>\n",
        "\n",
        "<!-- TEX CODE TO GENERATE THE ABOVE:\n",
        "\\begin{table}[]\n",
        "\\resizebox{1.00\\linewidth}{!}{\n",
        "\\begin{tabular}{llll}\n",
        "\\toprule\n",
        "\\textbf{Loss}    & \\textbf{Formula}     & \\textbf{Purpose}        & \\textbf{Domain} \\\\[1mm] \\midrule\n",
        "Squared loss & $\\mathcal{C}(y,~f(\\mathbf{x}))~=~(y-f(\\mathbf{x}))^2$ & Regression & $f(\\mathbf{x}) \\in \\mathbb{R}$ \\\\[2mm]\n",
        "Logistic loss & $\\mathcal{C}(y,~f(\\mathbf{x}))~= -y\\log(f(\\mathbf{x}))-(1-y)\\log(1-f(\\mathbf{x}))$ & Binary clf & $f(\\mathbf{x}) \\in [0,1]$ \\\\[2mm]\n",
        "Cross entropy & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k y_j\\log(f_j(\\mathbf{x})) $ & \\begin{tabular}[c]{@{}l@{}}Multi-class\\\\ classification\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in \\Delta^k$ \\\\[3.5mm]\n",
        "Multi-label logistic loss & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k(-y_j\\log(f_j(\\mathbf{x}))-(1-y_j)\\log(1-f_j(\\mathbf{x}))) $ & \\begin{tabular}[c]{@{}l@{}}Multi-label\\\\ classification\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in [0,1]^k$ \\\\[3.5mm]\n",
        "Multi-output squared loss & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k(y_j-f_j(\\mathbf{x}))^2 $ & \\begin{tabular}[c]{@{}l@{}}Multi-output\\\\ regression\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^k$ \\\\[2mm]\\bottomrule\n",
        "\\end{tabular}\n",
        "}\n",
        "\\end{table}\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-HCezsOMXXC"
      },
      "source": [
        "Keep in mind that all of the above methods usually fit a bias/intercept in addition to weights fitted on the input features.\n",
        "For an MLP, visually it would look like this:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/BioML-UGent/MLLS/main/11_intro_nns/biases.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDxgf-POMXXM"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>THOUGHT EXERCISE:</b>\n",
        "<p> How much weights does the model pictured above have (including biases)? </p>\n",
        "\n",
        "</div>\n",
        "\n",
        "Keep in mind that every layer consists of (1) a matrix multiplication of an input $\\mathbf{A} \\in \\mathbb{R}^{N, D}$ with a set of weights $\\mathbf{W} \\in \\mathbb{R}^{D, M}$ and (2) a non-linearity. With $D$ the number of input features/dimensions/nodes (including one node/feature for the intercept, see the following equation) and $M$ the number of output dimensions/nodes.\n",
        "\\begin{equation}\n",
        "\\mathbf{A}\\mathbf{W} =\n",
        "\\begin{bmatrix}\n",
        "1 & x_{1,1} & ...  & x_{1,D-1} & x_{1,D} \\\\\n",
        "1 & x_{1,1} & ... & x_{1,D-1} & x_{1,D} \\\\\n",
        "... & ... & ... & ... & ...\\\\\n",
        "1 & x_{N-1,1} & ... & x_{N-1,D-1} & x_{N-1,D} \\\\\n",
        "1 & x_{N,1} & ...  & x_{N,D-1} & x_{N,D} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "W_{1,1} & W_{1,1} & ...  & W_{1,M-1} & W_{1,M} \\\\\n",
        "W_{1,1} & W_{1,1} & ... & W_{1,M-1} & W_{1,M} \\\\\n",
        "... & ... & ... & ... & ...\\\\\n",
        "W_{D-1,1} & W_{D-1,1} & ... & W_{D-1,M-1} & W_{D-1,M} \\\\\n",
        "W_{D,1} & W_{D,1} & ...  & W_{D,M-1} & W_{D,M} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-9uaqpP-g6"
      },
      "source": [
        "The whole network is trained by first performing a forward pass to get predictions and compute a loss w.r.t. ground truth known labels, then doing backpropagation (essentially applying the chain rule of derivatives) to obtain the gradient of all weights w.r.t. the loss. These gradients are then used by gradient descent (or more modern variants such as Adam) to optimize the neural network.\n",
        "\n",
        "Training neural networks is (typically) more computationally demanding than more traditional machine learning methods, and we usually use neural networks when we have large datasets. For these two reasons, it is not a good idea to process the whole dataset at once in every training step/iteration. Therefore, to train the network, we process samples in batches, called **(mini-batch) stochastic gradient descent**. This allows for faster training and improved convergence of the loss during gradient descent. Advantages of stochastic gradient descent or other optimization algorithms for loss calculation are not discussed in this PC-lab, but have been [extensively discussed](https://ruder.io/optimizing-gradient-descent/) before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1IGpzXZMXXk"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piqgXwstMXXq"
      },
      "source": [
        "Dropout is a popular addition to neural networks. It is a form of regularization during which we stochastically deactivate a percentage of neurons in every training step by putting their activation to zero. This regularization only happens during training, as during testing we (usually) want deterministic outputs. Conceptually, it is similar to other regularization techniques such as ridge regression and subsampling of features in random forest training, in the sense that it will force our model to look at all features, because sometimes one single feature will not be available during training. The difference here is that we do it by stochastically putting nodes to zero during training, and that we can perform it not only on our input features, but also on our hidden nodes.\n",
        "\n",
        "Mathematically, it can be performed by simply sampling a boolean vector and doing element-wise multiplication.\n",
        "\n",
        "Visually, it would look a bit like this, where nodes in cyan are dropped out, and the associated cyan weights do not have any influence on training anymore (in that training step):\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/BioML-UGent/MLLS/main/11_intro_nns/dropout.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG3LPfPU-8ky"
      },
      "source": [
        "## 2 PyTorch\n",
        "\n",
        "To implement neural networks with more ease, a few high-level python libraries are available: (PyTorch, TensorFlow/keras, JAX, ...). These libraries provide functionality in terms of automatic differentiation (backprop), ready-to-use implementations for various layers, loss functions ...\n",
        "\n",
        "In this lab, we will use [PyTorch](https://pytorch.org). PyTorch is the most popular library for deep learning in academia as of today. For this course it offers the advantage that it has the most 'pythonic' syntax, to the point where almost all NumPy functions have a PyTorch counterpart.\n",
        "\n",
        "If you want to run this notebook locally, you can find the installation instructions for PyTorch [here](https://pytorch.org/get-started/locally/). Make sure to select the right installation options depending on your system (if you have a GPU or not).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1cmzGhe6kJK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKVm96u6BXev"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the fundamental data structures in PyTorch. They are analogous to NumPy arrays. The difference is that tensors can also run on GPU hardware. GPU hardware is optimized for many small computations. Matrix multiplications, the building blocks of all deep learning, run orders-of-magnitude faster on GPU than on CPU. Let's see how tensors are constructed and what we can do with them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJL-R3elBv9E"
      },
      "outputs": [],
      "source": [
        "x = [[5,8],[9,8]]\n",
        "print(torch.tensor(x))\n",
        "print(np.array(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOjuB-_7B-er"
      },
      "outputs": [],
      "source": [
        "x_numpy = np.array(x)\n",
        "print(torch.from_numpy(x_numpy))\n",
        "\n",
        "x_torch = torch.tensor(x)\n",
        "print(x_torch.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvQP_Y-HBEJt"
      },
      "outputs": [],
      "source": [
        "print(np.random.randn(8).shape)\n",
        "print(np.random.randn(8,50).shape)\n",
        "\n",
        "\n",
        "print(torch.randn(8).shape) # an alternative for .shape in PyTorch is .size()\n",
        "print(torch.randn(8,50).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grsQLtyKBGcq"
      },
      "outputs": [],
      "source": [
        "print(np.zeros((8,50)).shape)\n",
        "print(torch.zeros(8,50).shape) # works with 'ones' as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp-T8i15DXD7"
      },
      "source": [
        "In PyTorch, the standard data type for floats is `float32`, which is synonymous to `float` within its framework. `float64` is synonymous to `double`.\n",
        "This is different from the NumPy defaults and naming conventions: NumPy default data type for float is `float64`. Keep this in mind when converting numpy arrays to tensors and back!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e24OlYbC7nr"
      },
      "outputs": [],
      "source": [
        "print(np.zeros(8).dtype)\n",
        "print(torch.zeros(8).dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMr4OR05MXY6"
      },
      "source": [
        "Conversion of data types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFuID3EGDIea"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8)\n",
        "print(x.dtype)\n",
        "x = x.to(torch.float64)\n",
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhOAtkL-D269"
      },
      "source": [
        "`torch.long` is synonymous to `torch.int64`. The only difference between int32 and int64 is the amount of bytes with which you will store every integer. If you go up to very high numbers, you will get numerical overflow faster with more compressed data types. We recommend you to always use the defaults: `torch.long` and `torch.float`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAeLblceDTBq"
      },
      "outputs": [],
      "source": [
        "x = torch.randint(low=0, high=8, size=(8,), dtype=torch.int32)\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "x = x.to(torch.long)\n",
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNm0VKLzEbxH"
      },
      "source": [
        "Indexing and other operations work as in NumPy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWdkTEzXDtpq"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8,50,60)\n",
        "print(x.shape)\n",
        "print(x[:4,10:-10].shape)\n",
        "x[0,0,:10] = 0\n",
        "print(x[0,0,:16])\n",
        "\n",
        "print(torch.min(x), torch.max(x), torch.min(torch.abs(x)))\n",
        "# most of these functions are also tensor methods:\n",
        "print(x.min(), x.max(), x.abs().min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqK-1JtNMXZY"
      },
      "source": [
        "Joining tensors via concatenation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIZpsdFHEuTK"
      },
      "outputs": [],
      "source": [
        "print(x.shape)\n",
        "x_cat0 = torch.cat([x, x], dim=0)\n",
        "print(x_cat0.shape)\n",
        "x_cat1 = torch.cat([x, x, x], dim=1)\n",
        "print(x_cat1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPxk_yKfFgUi"
      },
      "source": [
        "Matrix multiplication: let's say we have an input `x`, consisting of 8 samples with 26 features, that we linearly combine with weights `w` to get a single output for every sample:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6LGBkLiFNXK"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8,26)\n",
        "w = torch.randn(26,1)\n",
        "\n",
        "y_hat = torch.matmul(x, w) # an alternative and equivalent syntax is x @ w\n",
        "print(y_hat)\n",
        "print(y_hat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az2LZ1hnGDds"
      },
      "source": [
        "Note that matrix multiplication is different from element-wise multiplication. For element-wise, `*` is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTgPXaUIMXZu"
      },
      "outputs": [],
      "source": [
        "x = torch.ones(8)\n",
        "print(x)\n",
        "x = x - 1.5\n",
        "print(x)\n",
        "x -= 1.5\n",
        "print(x)\n",
        "x += torch.randn(1)\n",
        "print(x)\n",
        "x += torch.randn(8)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S1kRG93MXZ4"
      },
      "source": [
        "Broadcasting works as in NumPy: [link](https://pytorch.org/docs/stable/notes/broadcasting.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnQkhCFlMXaA"
      },
      "source": [
        "Keep in mind, just like in NumPy, whatever you want to do with a tensor, there's probably an elegant operation for it implemented somewhere, you just have to look for it (on google and in the documentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEJoq1gsI56b"
      },
      "source": [
        "## 3 Building a neural network in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 The building blocks\n",
        "\n",
        "Neural networks are initialized through the use of class objects. You have encountered class objects already during this course: sklearn models are all class objects. The difference here is that we will code our own class first, before using it.\n",
        "\n",
        "Many of the functionalities necessary to create [**all types of neural networks**](http://www.asimovinstitute.org/neural-network-zoo/) have [**already been implemented**](http://pytorch.org/docs/master/nn.html).\n",
        "\n",
        "Let's inspect the most basic building blocks first: the [linear layer](https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear) and the [ReLU](https://pytorch.org/docs/master/generated/torch.nn.ReLU.html#torch.nn.ReLU)"
      ],
      "metadata": {
        "id": "khYlHomQnmKL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk2J6rxxMXaF"
      },
      "source": [
        "A linear layer is an object that will perform a matrix multiplication once called. Here, we instantiate such a layer with 20 input nodes and 40 output nodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66CEo4JuMXaI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "nn.Linear(20, 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI0RkwIXMXaO"
      },
      "source": [
        "Let's simulate some random data for this layer: A data set (or batch) with 16 samples and 20 features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27Qc4tKsMXaS"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(16, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz0B6EzmMXaY"
      },
      "source": [
        "Now let's use our linear layer on this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrSGoQcCMXac"
      },
      "outputs": [],
      "source": [
        "layer = nn.Linear(20, 40)\n",
        "print(x.shape)\n",
        "z = layer(x)\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0j01q3UMXah"
      },
      "source": [
        "What happens when we try to feed our layer an input with a different number of features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvAGyMa0MXan"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(16, 30)\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEQOYifSMXat"
      },
      "source": [
        "Let's see the ReLU in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CC_eT1wMXax"
      },
      "outputs": [],
      "source": [
        "relu = nn.ReLU()\n",
        "\n",
        "x = torch.randn(2, 4)\n",
        "print(x)\n",
        "z = relu(x)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS75ldCQMXa2"
      },
      "source": [
        "As you may have noticed, `nn.Module`s are class objects, a bit like scikit-learn models, that you instantiate and then call.\n",
        "\n",
        "You can chain `nn.Module`s with the use of `nn.Sequential`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r3NjqwzMXa9"
      },
      "outputs": [],
      "source": [
        "linear_and_relu = nn.Sequential(nn.Linear(20, 40), nn.ReLU())\n",
        "\n",
        "x = torch.randn(16, 20)\n",
        "z = linear_and_relu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb07hQ5fMXbG"
      },
      "source": [
        "Or even longer constructs:\n",
        "\n",
        "Always keep in mind what happens with the dimensions of your input and outputs with every layer, if your first layer outputs 40 features/nodes/hidden dimensions, then logically the next will have to take 40 as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl_95q5iMXbM"
      },
      "outputs": [],
      "source": [
        "a_whole_damn_network = nn.Sequential(\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1)\n",
        "    )\n",
        "\n",
        "x = torch.randn(16, 128)\n",
        "z = a_whole_damn_network(x)\n",
        "print(z.shape)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qODrbnv-MXbT"
      },
      "source": [
        "The output $z$ that we now obtain after this whole network are called **logits**. They are the real-numbered $\\mathbb{R}$ outputs that we obtain at the end of the network before our last activation function. This last activation function will be a, depending on the task at hand, sigmoid, softmax, or nothing at all for regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj5XMc9sMXba"
      },
      "source": [
        "Similar implementations exist for Dropout and Normalization layers in PyTorch, we invite you to look them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErcZ92U3MXbe"
      },
      "source": [
        "### 3.2 Class object neural networks and hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT_rF-1cMXbi"
      },
      "source": [
        "We've seen how to implement a neural network using PyTorch `nn.Sequential`. It is more flexible however to write our own model class. This allows us to have more control over which operations we use and define our own hyperparameters. To make a PyTorch model, we specify our class object to be a submodule of `nn.Module` and inherit its methods via `super().__init__()`. Further, we specify all necessary attributes (such as layers) in our `__init__` function (executed upon initialization) and implement a `forward` function which will be executed when we call the object after being initialized.\n",
        "\n",
        "The following code shows two examples, the first one of a very basic construction of a neural network without hyperparameters. The other one shows the same network, but where we set up our `__init__` function to process hyperparameters as input arguments. We can for example specify a hyperparameter whether we want to use dropout or not. (We could go even further to have an extra hyperparameter specifying the probability of dropout, ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCWUPuY_I-nq"
      },
      "outputs": [],
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(50, 40)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "        self.layer2 = nn.Linear(40, 20)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "        self.layer3 = nn.Linear(20, 10)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "        self.layer4 = nn.Linear(10, 5)\n",
        "        # Think again: why do we not want a relu and dropout after our last layer again?\n",
        "\n",
        "    def forward(self, x):\n",
        "        # call them in separate lines:\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # or together:\n",
        "        x = self.dropout2(self.relu2(self.layer2(x)))\n",
        "        x = self.dropout3(self.relu3(self.layer3(x)))\n",
        "\n",
        "        # we could've also wrapped everything in a nn.Sequential ..\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "class HyperparameterModel(nn.Module):\n",
        "    def __init__(self, dimensions_from_input_to_output = [50, 40, 20, 10, 5], dropout = True):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        # iterate through all layers:\n",
        "        for i in range(len(dimensions_from_input_to_output) - 2):\n",
        "            layer = nn.Linear(dimensions_from_input_to_output[i], dimensions_from_input_to_output[i + 1])\n",
        "            layers.append(layer)\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout == True:\n",
        "                layers.append(nn.Dropout(0.2))\n",
        "\n",
        "\n",
        "        # the last layer separate from the loop because we don't want a ReLU and dropout after the last layer\n",
        "        layer = nn.Linear(dimensions_from_input_to_output[i+1], dimensions_from_input_to_output[i + 2])\n",
        "        layers.append(layer)\n",
        "\n",
        "        # wrap the layers in a sequential\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Havh_KMXbs"
      },
      "source": [
        "Testing out the basic model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj1h-SUXMXbw"
      },
      "outputs": [],
      "source": [
        "net = BasicModel()\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGo96VzdMXb1"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 50)\n",
        "y = net(x)\n",
        "print(y)\n",
        "print(y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dot4uBfzMXb-"
      },
      "source": [
        "Testing out the hyperparameter model, when no arguments are specified the default ones are chosen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlVAqCvgMXcC"
      },
      "outputs": [],
      "source": [
        "net = HyperparameterModel()\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exSn3_hWMXcI"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 50)\n",
        "y = net(x)\n",
        "print(y)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN6ZLONoMXcP"
      },
      "source": [
        "Or by specifying hyperparameters, the following code shows a bit of a deeper model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFbjxV_9MXcS"
      },
      "outputs": [],
      "source": [
        "net = HyperparameterModel(dimensions_from_input_to_output = [50, 160, 80, 40, 20, 10, 5], dropout = False)\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35MBDkKZMXcW"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 50)\n",
        "y = net(x)\n",
        "print(y)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci8sWl7RMXcZ"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b>\n",
        "<p> Copy over any of the two networks above (preferably the one you understand best), and modify it to take as input 784 features and as output 10 nodes. We will use this network later in the PC lab. You can test your network for bugs using some randomly generated data (as above).</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuJm_kzwMXce"
      },
      "outputs": [],
      "source": [
        "######## YOUR CODE HERE #########\n",
        "\n",
        "\n",
        "#################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pqMG8vMXci"
      },
      "source": [
        "### 3.3 Data and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5dpoMHtMXcj"
      },
      "source": [
        "You may have noticed that the outputs of your model have a `grad_fn` attribute. This grad function will be used by PyTorch automatic differentation engine to perform backward passes and compute gradients for every parameter with respect to the loss/cost function.\n",
        "\n",
        "Now that we have our model and know that PyTorch will automatically compute gradients for us, we can move on to train a model. To do this, we need a couple more things:\n",
        "\n",
        "The most basic blueprint of PyTorch model training consists of\n",
        "- Get your data\n",
        "- Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "- Instantiate the model\n",
        "- Instantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "- Iterate through your training data, for every batch:\n",
        "    - reset the gradients\n",
        "    - do forward pass\n",
        "    - compute loss\n",
        "    - backward pass\n",
        "    - update parameters\n",
        "\n",
        "(Optionally):\n",
        "- After every full iteration through all training data samples (called an epoch), loop through all batches of validation data:\n",
        "    - forward pass\n",
        "    - compute loss and validation scores\n",
        "\n",
        "In this way, we can monitor how good our model is performing on left-out validation data during training, this is used in early stopping. Notably, we do not compute and reset gradients and update parameters during our validation iterations, because we do not want to fit our model on this data.\n",
        "\n",
        "\n",
        "Let's start with step one: get your data and wrap them in data loaders. We will first illustrate how we can convert our usual pandas or numpy datasets to be compatible with PyTorch with some random data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZG5Ikr8MXck"
      },
      "outputs": [],
      "source": [
        "X_train = np.random.randn(100, 784)\n",
        "y_train = np.random.randint(low = 0, high = 10, size = (100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bbTYpxMXcm"
      },
      "outputs": [],
      "source": [
        "X_train = torch.from_numpy(X_train)\n",
        "print(X_train.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlAd2-iMXcq"
      },
      "source": [
        "Remember to look at your data types: by default NumPy is `float64`, but if you instantiate a model, by default it will have weights in `float32`. It is therefore advised to convert your data to `float32`. In PyTorch, simply `float` is shorthand for `float32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF43PgTRMXcr"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.float()\n",
        "# Equivalent: X_train.to(torch.float) or X_train.to(torch.float32)\n",
        "print(X_train.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMTh2qpAMXcu"
      },
      "outputs": [],
      "source": [
        "y_train = torch.tensor(y_train)\n",
        "print(y_train.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnI0TnDQMXc1"
      },
      "source": [
        "Now that we have our X_train and y_train as tensors, we can make them PyTorch-ready by wrapping them in a PyTorch dataset, and then by wrapping them in a DataLoader, which will create batches for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMRu_hLsMXc4"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quJWFJxiMXc5"
      },
      "source": [
        "Now we can use our train_dataloader as such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv5E6nmXMXc6"
      },
      "outputs": [],
      "source": [
        "for batch in train_dataloader:\n",
        "    X, y = batch\n",
        "    print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb8HQt4MXc_"
      },
      "source": [
        "As you can see, we can iterate through our batches by use of a for loop, and it will spit out a training batch consisting of a list of X and y tensors. We can also test out code by isolating one training batch like this (only necessary for testing out code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk4l7myPMXdF"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifxPbgJjMXdJ"
      },
      "source": [
        "For the remainder of the PC lab, we will work with the MNIST dataset included in `torchvision`. (If you're running this code locally, you may have to pip install torchvision)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as-YG3-_MXdM"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "X_train = train_data.data\n",
        "y_train = train_data.targets\n",
        "\n",
        "X_test = test_data.data\n",
        "y_test = test_data.targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8hdN8G5MXdQ"
      },
      "outputs": [],
      "source": [
        "print('shapes:')\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "print('first training image tensor:')\n",
        "print(X_train[0])\n",
        "print('first five labels:')\n",
        "print(y_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA03h75wMXdV"
      },
      "source": [
        "Our data is images, each data sample has $28 \\times 28$ input features, signifying the pixels. In order to feed this data to our model, we will need to flatten these features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgNA_Y8gMXdW"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(-1, 28 * 28)\n",
        "X_test = X_test.reshape(-1, 28 * 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQryYAP6MXda"
      },
      "source": [
        "In addition, the grayscale values of our images go from 0 to 255. It is perhaps good practice to min-max standardize these numbers by dividing through 255:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUXvQOqXMXdb"
      },
      "outputs": [],
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai8QjbZ9MXdh"
      },
      "source": [
        "Finally, let's check our datatypes to see if everything is looking good to go:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n9Pg1VxMXdj"
      },
      "outputs": [],
      "source": [
        "X_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3mE5c6nMXdk"
      },
      "source": [
        "Let's split up our training set in a training and validation set and finally wrap our data in a data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9DQFGa3MXdl"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "train_indices, val_indices = np.split(np.random.permutation(len(X_train)), [int(len(X_train)*0.8)])\n",
        "X_val = X_train[val_indices]\n",
        "y_val = y_train[val_indices]\n",
        "X_train = X_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, pin_memory=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4GXH_2MXdn"
      },
      "source": [
        "Let's visualize a random batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn3rRoSTMXdq"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "X_batch, y_batch = batch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(10, 8))\n",
        "cols, rows = 4, 4\n",
        "for i in range(cols * rows):\n",
        "    img, label = X_batch[i], y_batch[i]\n",
        "    figure.add_subplot(rows, cols, i+1)\n",
        "    plt.title(label.item())\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.reshape(-1, 28, 28).squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGmLYd15MXdw"
      },
      "source": [
        "Now that we have our data ready, let's reiterate our PyTorch model blueprint:\n",
        "\n",
        "The most basic blueprint of PyTorch model training consists of\n",
        "- Get your data\n",
        "- Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "- Instantiate the model\n",
        "- Instantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "- Iterate through your training data, for every batch:\n",
        "    - reset the gradients\n",
        "    - do forward pass\n",
        "    - compute loss\n",
        "    - backward pass\n",
        "    - update parameters\n",
        "\n",
        "(Optionally):\n",
        "- After every full iteration through all training data samples (called an epoch), loop through all batches of validation data:\n",
        "    - forward pass\n",
        "    - compute loss and validation scores\n",
        "\n",
        "\n",
        "In one of the previous exercises, we already implemented a model compatible with MNIST: 784 input features and 10 output nodes (one for each class). Hence, we can move on to loss function and optimizers. For multi-class classification of the digits, we will need to use the [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss). Take a look at what kind of inputs this loss function expects. According to the documentation: `The input is expected to contain raw, unnormalized scores for each class.` Meaning that we can pass logits directly to this loss function, and we do not have to apply a softmax operation ourselves.\n",
        "\n",
        "For optimizer choice, we can choose [vanilla gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) or the nowadays-industry-standard [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam), which is itself a pimped version of stochastic gradient descent (with momentum).\n",
        "\n",
        "Take note that we can also specify our desired learning rate in our model. This learning should almost always be tuned as it will influence how fast our model trains but also convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kZwMU_IMXd1"
      },
      "outputs": [],
      "source": [
        "model = # your model from previous exercises here.\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) # SGD = stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGrEVraTMXd7"
      },
      "source": [
        "Now we're ready to perform training. We'll build up the training loop step-wise in the following codeblocks. Let's first start with passing one batch to the model, computing the loss, performing backpropagation and updating the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROoh6JGdMXd8"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "X_batch, y_batch = batch\n",
        "\n",
        "y_hat_batch = model(X_batch)\n",
        "\n",
        "loss = loss_function(y_hat_batch, y_batch) # Compute loss\n",
        "\n",
        "loss.backward()   # Calculate gradients\n",
        "optimizer.step()   # Update weights using defined optimizer\n",
        "\n",
        "print(X_batch.shape, y_batch.shape)\n",
        "print(y_hat_batch.shape)\n",
        "print(\"Outputs as logits, first two samples:\")\n",
        "print(y_hat_batch[:2])\n",
        "print('loss:', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hlu1gnRMXd_"
      },
      "source": [
        "Everytime we perform a training step, we should reset the gradients so that the gradients computed on the previous batch do not influence the next. We can do this by calling `.zero_grad()` on our optimizer. In practice, it is most safe to call this before every forward pass. So if we train a complete epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiDrim2GMXeE"
      },
      "outputs": [],
      "source": [
        "all_losses = []\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    X_batch, y_batch = batch\n",
        "\n",
        "    y_hat_batch = model(X_batch)\n",
        "\n",
        "    loss = loss_function(y_hat_batch, y_batch) # Compute loss\n",
        "\n",
        "    loss.backward()   # Calculate gradients\n",
        "    optimizer.step()   # Update weights using defined optimizer\n",
        "\n",
        "    all_losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXPQQS4QMXeR"
      },
      "source": [
        "Plotting the loss function of every batch during one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgO5nCJJMXeZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(all_losses)), all_losses)\n",
        "smoothed_losses = np.convolve(all_losses, np.ones(50)/50, mode = \"valid\")\n",
        "plt.plot(np.arange(len(smoothed_losses)), smoothed_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD36b8QPMXee"
      },
      "source": [
        "We have evaluated the training progress during one epoch on our training set. What if we want to see the performance of the validation set during training, so that we can see if/when the model starts overfitting? It is common practice to perform a pass through the whole validation set after every training epoch. However, we should not compute gradients now, we can block automatic gradient computation using `torch.no_grad()`. Also, we need to tell PyTorch to put our model in evaluation mode (so it does not perform dropout anymore, etc...). After the validation epoch, we should put the model back to training mode again. The code should look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw0wKUcNMXeh"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "true_labels = []\n",
        "losses = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        X_batch, y_batch = batch\n",
        "\n",
        "        y_hat_batch = model(X_batch)\n",
        "\n",
        "        loss = loss_function(y_hat_batch, y_batch)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        predictions.append(y_hat_batch)\n",
        "        true_labels.append(y_batch)\n",
        "\n",
        "model.train()\n",
        "\n",
        "predictions = torch.cat(predictions)\n",
        "true_labels = torch.cat(true_labels)\n",
        "accuracy = (true_labels == predictions.argmax(-1)).sum().item() / len(predictions)\n",
        "\n",
        "print(accuracy)\n",
        "print(np.mean(losses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f_yo3RiMXeq"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b>\n",
        "<p> Using the code above, put it all together to train a model for multiple epochs. After every epoch, print or save some training and validation statistics.\n",
        "\n",
        "Monitor how good your model is training. What things could you change? In particular, try the Adam optimizer or try using gradient descent with the momentum argument. How does this influence training speed? What is it doing? Try tweaking the learning rate.\n",
        "\n",
        "You should be able to obtain an accuracy of +- 96% </p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj0vgM2WMXeu"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "model =  # your model from previous exercises here.\n",
        "\n",
        "# loss function & optimizer\n",
        "\n",
        "for i in range(1, N_EPOCHS + 1):\n",
        "\n",
        "    # train loop\n",
        "\n",
        "    # eval loop\n",
        "\n",
        "    # record or print some variables that you want to keep track of during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04jmvNeBMXe0"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b>\n",
        "<p> After training, what is your performance on the test dataset? </p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J8XfWlPMXe2"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUoF-7B2MXfS"
      },
      "source": [
        "### Extra: Using GPUs\n",
        "\n",
        "Matrix multiplication run orders of magnitude faster on GPU hardware. If you have a local GPU and have installed a PyTorch version with GPU, you should be able to run this code locally. Otherwise, In Google Colab, you can request access to a GPU via `Runtime > Change runtime type > Hardware accelerator = GPU`.\n",
        "\n",
        "Briefly, the steps needed to train on GPUs consist of\n",
        "1. Putting your model on the GPU\n",
        "2. During your training loop, putting every batch on the GPU before the forward pass\n",
        "3. If you have a validation loop, doing the same there for every batch.\n",
        "4. If you have variables that you will use after training (e.g. predictions on the validation set), remember to return this back to the CPU, as GPUs have limited memory.\n",
        "\n",
        "In PyTorch, we put variables and models on the GPU by specifying their 'device' to be 'cuda' (the parallel computing platform for nvidia GPUs that PyTorch uses).\n",
        "\n",
        "The following code illustrates how to train your models on GPU hardware:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4q8iNSBMXfT"
      },
      "outputs": [],
      "source": [
        "X, y = next(iter(train_dataloader))\n",
        "\n",
        "model = model.to('cuda')\n",
        "print(X.device)\n",
        "X = X.to('cuda')\n",
        "print(X.device)\n",
        "\n",
        "y_hat = model(X)\n",
        "\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLAm5JACMXfU"
      },
      "source": [
        "We encourage you to try out training on GPUs during the next PC lab(s)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PClab_intro_nns.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}