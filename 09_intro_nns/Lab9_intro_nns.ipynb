{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjxvlCzY6h4t"
      },
      "source": [
        "<img src='https://analyticsindiamag.com/wp-content/uploads/2022/04/Screenshot-2022-04-06-at-9.55.11-PM.png' width=750>\n",
        "\n",
        "*Images generated by a deep neural network that interprets text to generate images (DALL·E 2)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xqzgGkp6Z0a"
      },
      "source": [
        "# PC Lab 9: Introduction to Neural Networks \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNEdF_h4-AmQ"
      },
      "source": [
        "Deep learning is the subfield of machine learning that concerns neural networks with representation learning capabilities. As of recent years, it is arguably the most quickly growing field within machine learning, enjoying major breakthroughs every year. Listing a couple ones from last year(s): ChatGPT, AlphaFold v2, DALL·E 2, AlphaZero. Although the popularity of neural nets is a recent phenomenon, they were first described by Warren McCulloch and Walter Pitts in 1943. Early progress in training competitive neural networks was stalled by a multitude of reasons, such as the limited computer resources, sub-optimal network architectures and the use of smaller datasets. In this PC-lab we will introduce you to the basics of implementing a neural network using contemporary practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jqUbY3CO_fY"
      },
      "source": [
        "## 1. Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19SzILbZQkIL"
      },
      "source": [
        "### From linear models to neurons to neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CX3qibbPBs7"
      },
      "source": [
        "The core unit of every (artificial) neural network is considered the neuron. Every neuron can be observed as a linear combination of **one or more inputs** $\\mathbf{a}$ with weights $\\mathbf{w}$ (and optionally adding a **bias** $b$), outputting a **single output** $z$:\n",
        "\n",
        "$$z = \\sum\\limits_{i=1}^{n}(a_iw_i) + b,$$\n",
        "\n",
        "equivalently written as dot product:\n",
        "\n",
        "$$z = \\mathbf{a} \\cdot \\mathbf{w} + b.$$\n",
        "\n",
        "When multiple neurons are placed next to each other, we get multiple output neurons, this is called a **layer**. Mathematically, our weights and biases (intercepts) now become a matrix and vector respectively, and we obtain as output a vector $\\mathbf{z}$:\n",
        "\n",
        "$\\mathbf{z} = \\mathbf{a} \\cdot \\mathbf{W} + \\mathbf{b}.$\n",
        "\n",
        "Multiple **layers** can be stacked sequentially to make up a deep neural network. Performing this gives us a big linear model, because multiple matrix multiplications performed consecutively can also be written as just one: i.e. $\\mathbf{x} \\cdot \\mathbf{W_1} \\cdot \\mathbf{W_2} ...$ could also be written as just $\\mathbf{x} \\cdot \\mathbf{W_{all}}$. To obtain the *deep learning magic*, we need to make the whole thing **non-linear**. We do this by adding **activation functions** after every (hidden) layer. The most classical activation is the sigmoid activation $\\sigma()$, used also in logistic regression. Nowadays, we usually opt for a more simple activation function: the **ReLU** $\\texttt{ReLU}(z) = max(0,z)$. This function has the favorable property that its derivative is very efficient to compute (1 when $z$ is positive, 0 when it is negative). It also acts as a switch: a neuron will have a \"dead\" ($0$) activation whenever $z$ is negative.\n",
        "\n",
        "For the output layer of a neural network: our activation depends on the task at hand. For binary classification, we use sigmoid to constrain our output between 0 and 1. For multi-class, we use a softmax operation so the output of all neurons sums to 1. For regression, we simply do not use an activation or a custom one depending on your data: if you already know that your outputs can't be negative but can take all positive numbers ($\\mathbb{R}^+$), then maybe a ReLU activation in the output nodes makes sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o7g3tpiMXW4"
      },
      "source": [
        "In order to build more intuition for neural networks: consider the following figure where we \"visually build up\" a neural network starting from linear regression with four input features (**a**), to logistic regression (**b**), to an archetypical output neuron with ReLU activation (**c**). For multi-output settings, we visualize multi-output regression (**d**), multi-label classification (more than one class can be 1 in a sample) (**e**), and multi-class classification via softmax (**f**). Finally, a simple neural network with two hidden layers for binary classification (sigmoid output head) is shown under **g**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/tfmortie/mlmust/main/09_intro_nns/lr2nn.png'>\n",
        "\n",
        "**This figure makes it crystal clear that the most simple neural network is just a bunch of linear regressions stacked on top of eachother with non-linearities in between.** More advanced neural network architectures exist that modify how we make information flow between inputs. In this example, everything is just connected to everything with linear weights. This type of neural network is what we call an **MLP** or a **multi-layer perceptron**.\n",
        "\n",
        "Depending on the type of output, different loss functions may be used:\n",
        "<img src='https://raw.githubusercontent.com/tfmortie/mlmust/main/11_intro_nns/lossfunctions.png' width=800>\n",
        "\n",
        "<!-- TEX CODE TO GENERATE THE ABOVE:\n",
        "\\begin{table}[]\n",
        "\\resizebox{1.00\\linewidth}{!}{\n",
        "\\begin{tabular}{llll}\n",
        "\\toprule\n",
        "\\textbf{Loss}    & \\textbf{Formula}     & \\textbf{Purpose}        & \\textbf{Domain} \\\\[1mm] \\midrule\n",
        "Squared loss & $\\mathcal{C}(y,~f(\\mathbf{x}))~=~(y-f(\\mathbf{x}))^2$ & Regression & $f(\\mathbf{x}) \\in \\mathbb{R}$ \\\\[2mm]\n",
        "Logistic loss & $\\mathcal{C}(y,~f(\\mathbf{x}))~= -y\\log(f(\\mathbf{x}))-(1-y)\\log(1-f(\\mathbf{x}))$ & Binary clf & $f(\\mathbf{x}) \\in [0,1]$ \\\\[2mm]\n",
        "Cross entropy & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k y_j\\log(f_j(\\mathbf{x})) $ & \\begin{tabular}[c]{@{}l@{}}Multi-class\\\\ classification\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in \\Delta^k$ \\\\[3.5mm]\n",
        "Multi-label logistic loss & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k(-y_j\\log(f_j(\\mathbf{x}))-(1-y_j)\\log(1-f_j(\\mathbf{x}))) $ & \\begin{tabular}[c]{@{}l@{}}Multi-label\\\\ classification\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in [0,1]^k$ \\\\[3.5mm]\n",
        "Multi-output squared loss & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k(y_j-f_j(\\mathbf{x}))^2 $ & \\begin{tabular}[c]{@{}l@{}}Multi-output\\\\ regression\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^k$ \\\\[2mm]\\bottomrule\n",
        "\\end{tabular}\n",
        "}\n",
        "\\end{table}\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-HCezsOMXXC"
      },
      "source": [
        "Keep in mind that all of the above methods usually fit a bias/intercept in addition to weights fitted on the input features.\n",
        "For an MLP, visually it would look like this:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/tfmortie/mlmust/main/09_intro_nns/biases.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDxgf-POMXXM"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>THOUGHT EXERCISE:</b> **How much weights does the model pictured above have (including biases)?**\n",
        "</div>\n",
        "\n",
        "Keep in mind that every layer consists of (1) a matrix multiplication of an input $\\mathbf{A} \\in \\mathbb{R}^{N, D}$ with a set of weights $\\mathbf{W} \\in \\mathbb{R}^{D, M}$ and (2) a non-linearity. With $D$ the number of input features/dimensions/nodes (including one node/feature for the intercept, see the following equation) and $M$ the number of output dimensions/nodes.\n",
        "\\begin{equation}\n",
        "\\mathbf{A}\\mathbf{W} =\n",
        "\\begin{bmatrix}\n",
        "1 & x_{1,1} & ...  & x_{1,D-1} & x_{1,D} \\\\\n",
        "1 & x_{1,1} & ... & x_{1,D-1} & x_{1,D} \\\\\n",
        "... & ... & ... & ... & ...\\\\\n",
        "1 & x_{N-1,1} & ... & x_{N-1,D-1} & x_{N-1,D} \\\\\n",
        "1 & x_{N,1} & ...  & x_{N,D-1} & x_{N,D} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "W_{1,1} & W_{1,1} & ...  & W_{1,M-1} & W_{1,M} \\\\\n",
        "W_{1,1} & W_{1,1} & ... & W_{1,M-1} & W_{1,M} \\\\\n",
        "... & ... & ... & ... & ...\\\\\n",
        "W_{D-1,1} & W_{D-1,1} & ... & W_{D-1,M-1} & W_{D-1,M} \\\\\n",
        "W_{D,1} & W_{D,1} & ...  & W_{D,M-1} & W_{D,M} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-9uaqpP-g6"
      },
      "source": [
        "The whole network is trained by first performing a forward pass to get predictions and to compute a loss w.r.t. ground-truth labels, then doing backpropagation (essentially applying the chain rule of derivatives) to obtain the gradient of all weights w.r.t. the loss. These gradients are then used by gradient descent (or more modern variants such as Adam) to optimize the neural network.\n",
        "\n",
        "Training neural networks is (typically) more computationally demanding than more traditional machine learning methods, and we usually use neural networks when we have large datasets. For these two reasons, it is not a good idea to process the whole dataset at once in every training step/iteration. Therefore, to train the network, we process samples in batches, called **(mini-batch) stochastic gradient descent**. This allows for faster training and improved convergence of the loss during gradient descent. Advantages of stochastic gradient descent or other optimization algorithms for loss calculation are not discussed in this PC-lab, but have been [extensively discussed](https://ruder.io/optimizing-gradient-descent/) before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1IGpzXZMXXk"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piqgXwstMXXq"
      },
      "source": [
        "Dropout is a popular addition to neural networks. It is a form of regularization during which we stochastically deactivate a percentage of neurons in every training step by putting their activation to zero. This regularization only happens during training, as during testing we (usually) want deterministic outputs. Conceptually, it is similar to other regularization techniques such as ridge regression and subsampling of features in random forest training, in the sense that it will force our model to look at all features, because sometimes one single feature will not be available during training. The difference here is that we do it by stochastically putting nodes to zero during training, and that we can perform it not only on our input features, but also on our hidden nodes.\n",
        "\n",
        "Mathematically, it can be performed by simply sampling a boolean vector and doing element-wise multiplication.\n",
        "\n",
        "Visually, it would look a bit like this, where nodes in cyan are dropped out, and the associated cyan weights do not have any influence on training anymore (in that training step):\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/tfmortie/mlmust/main/09_intro_nns/dropout.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG3LPfPU-8ky"
      },
      "source": [
        "## 2. PyTorch\n",
        "\n",
        "To implement neural networks with more ease, a few high-level Python libraries are available: PyTorch, TensorFlow/Keras, JAX, etc. In this lab, we will use [PyTorch](https://pytorch.org). PyTorch is the most popular library for deep learning in academia as of today. For this course it offers the advantage that it has the most 'pythonic' syntax, to the point where almost all NumPy functions have a PyTorch counterpart.\n",
        "\n",
        "If you want to run this notebook locally, you can find the installation instructions for PyTorch [here](https://pytorch.org/get-started/locally/). Make sure to select the right installation options depending on your system (if you have a GPU or not).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1cmzGhe6kJK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKVm96u6BXev"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the fundamental data structures in PyTorch. They are similar to NumPy arrays. The difference is that tensors can also run on GPU hardware. GPU hardware is optimized for many small computations. Matrix multiplications, the building blocks of all deep learning, run orders-of-magnitude faster on GPU than on CPU. Let's see how tensors are constructed and what we can do with them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJL-R3elBv9E"
      },
      "outputs": [],
      "source": [
        "x = [[5,8],[9,8]]\n",
        "print(torch.tensor(x))\n",
        "print(np.array(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOjuB-_7B-er"
      },
      "outputs": [],
      "source": [
        "x_numpy = np.array(x)\n",
        "print(torch.from_numpy(x_numpy))\n",
        "\n",
        "x_torch = torch.tensor(x)\n",
        "print(x_torch.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvQP_Y-HBEJt"
      },
      "outputs": [],
      "source": [
        "print(np.random.randn(8).shape)\n",
        "print(np.random.randn(8,50).shape)\n",
        "\n",
        "\n",
        "print(torch.randn(8).shape) # an alternative for .shape in PyTorch is .size()\n",
        "print(torch.randn(8,50).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grsQLtyKBGcq"
      },
      "outputs": [],
      "source": [
        "print(np.zeros((8,50)).shape)\n",
        "print(torch.zeros(8,50).shape) # works with 'ones' as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp-T8i15DXD7"
      },
      "source": [
        "In PyTorch, the standard data type for floats is `float32`, which is synonymous to `float` within its framework. `float64` is synonymous to `double`.\n",
        "This is different from the NumPy defaults and naming conventions: NumPy default data type for float is `float64`. Keep this in mind when converting NumPy arrays to tensors and back!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e24OlYbC7nr"
      },
      "outputs": [],
      "source": [
        "print(np.zeros(8).dtype)\n",
        "print(torch.zeros(8).dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMr4OR05MXY6"
      },
      "source": [
        "Conversion of data types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFuID3EGDIea"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8)\n",
        "print(x.dtype)\n",
        "x = x.to(torch.float64)\n",
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhOAtkL-D269"
      },
      "source": [
        "`torch.long` is synonymous to `torch.int64`. The only difference between int32 and int64 is the amount of bytes with which you will store every integer. If you go up to very high numbers, you will get numerical overflow faster with more compressed data types. We recommend you to always use the defaults: `torch.long` and `torch.float`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAeLblceDTBq"
      },
      "outputs": [],
      "source": [
        "x = torch.randint(low=0, high=8, size=(8,), dtype=torch.int32)\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "x = x.to(torch.long)\n",
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNm0VKLzEbxH"
      },
      "source": [
        "Indexing and other operations work similarly as for NumPy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWdkTEzXDtpq"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8,50,60)\n",
        "print(x.shape)\n",
        "print(x[:4,10:-10].shape)\n",
        "x[0,0,:10] = 0\n",
        "print(x[0,0,:16])\n",
        "\n",
        "print(torch.min(x), torch.max(x), torch.min(torch.abs(x)))\n",
        "# most of these functions are also tensor methods:\n",
        "print(x.min(), x.max(), x.abs().min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqK-1JtNMXZY"
      },
      "source": [
        "Joining tensors via concatenation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIZpsdFHEuTK"
      },
      "outputs": [],
      "source": [
        "print(x.shape)\n",
        "x_cat0 = torch.cat([x, x], dim=0)\n",
        "print(x_cat0.shape)\n",
        "x_cat1 = torch.cat([x, x, x], dim=1)\n",
        "print(x_cat1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPxk_yKfFgUi"
      },
      "source": [
        "Matrix multiplication: let's say we have an input `x`, consisting of 8 samples with 26 features, which is linearly combined with weights `w` in order to get a single output for every sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6LGBkLiFNXK"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8,26)\n",
        "w = torch.randn(26,1)\n",
        "\n",
        "y_hat = torch.matmul(x, w) # an alternative and equivalent syntax is x @ w\n",
        "print(y_hat)\n",
        "print(y_hat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az2LZ1hnGDds"
      },
      "source": [
        "Note that matrix multiplication is different from element-wise multiplication. For element-wise, `*` is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTgPXaUIMXZu"
      },
      "outputs": [],
      "source": [
        "x = torch.ones(8)\n",
        "print(x)\n",
        "x = x - 1.5\n",
        "print(x)\n",
        "x -= 1.5\n",
        "print(x)\n",
        "x += torch.randn(1)\n",
        "print(x)\n",
        "x += torch.randn(8)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEJoq1gsI56b"
      },
      "source": [
        "## 3. Building a neural network in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khYlHomQnmKL"
      },
      "source": [
        "### 3.1 The building blocks\n",
        "\n",
        "Neural networks are initialized through the use of class objects. You have encountered class objects already during this course: Scikit-learn models are all class objects. The difference here is that we will code our own class first, before using it.\n",
        "\n",
        "Many of the functionalities necessary to create [**all types of neural networks**](http://www.asimovinstitute.org/neural-network-zoo/) have [**already been implemented**](http://pytorch.org/docs/master/nn.html).\n",
        "\n",
        "Let's inspect the most basic building blocks first: the [linear layer](https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear) and the [ReLU](https://pytorch.org/docs/master/generated/torch.nn.ReLU.html#torch.nn.ReLU) activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk2J6rxxMXaF"
      },
      "source": [
        "A linear layer is an object that will perform a matrix multiplication once called. Here, we instantiate such a layer with 20 input nodes and 40 output nodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66CEo4JuMXaI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "nn.Linear(20, 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI0RkwIXMXaO"
      },
      "source": [
        "Let's simulate some random data for this layer: a dataset (or batch) with 16 samples and 20 features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27Qc4tKsMXaS"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(16, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz0B6EzmMXaY"
      },
      "source": [
        "Now let's use our linear layer on this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrSGoQcCMXac"
      },
      "outputs": [],
      "source": [
        "layer = nn.Linear(20, 40)\n",
        "print(x.shape)\n",
        "z = layer(x)\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0j01q3UMXah"
      },
      "source": [
        "What happens when we try to feed our layer an input with a different number of features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvAGyMa0MXan"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(16, 30)\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEQOYifSMXat"
      },
      "source": [
        "Let's see the ReLU in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CC_eT1wMXax"
      },
      "outputs": [],
      "source": [
        "relu = nn.ReLU()\n",
        "\n",
        "x = torch.randn(2, 4)\n",
        "print(x)\n",
        "z = relu(x)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS75ldCQMXa2"
      },
      "source": [
        "As you may have noticed, `nn.Module`s are class objects, similarly as Scikit-learn models that you instantiate and then call.\n",
        "\n",
        "You can chain `nn.Module`s with the use of `nn.Sequential`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r3NjqwzMXa9"
      },
      "outputs": [],
      "source": [
        "linear_and_relu = nn.Sequential(nn.Linear(20, 40), nn.ReLU())\n",
        "\n",
        "x = torch.randn(16, 20)\n",
        "z = linear_and_relu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb07hQ5fMXbG"
      },
      "source": [
        "Or even longer constructs:\n",
        "\n",
        "Always keep in mind what happens with the dimensions of your input and outputs with every layer. If your first layer outputs 40 features/nodes/hidden dimensions, then logically the next will have to take 40 as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl_95q5iMXbM"
      },
      "outputs": [],
      "source": [
        "a_whole_damn_network = nn.Sequential(\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1)\n",
        "    )\n",
        "\n",
        "x = torch.randn(16, 128)\n",
        "z = a_whole_damn_network(x)\n",
        "print(z.shape)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qODrbnv-MXbT"
      },
      "source": [
        "The output $z$ that we now obtain after this whole network are called **logits**. They are the real-numbered $\\mathbb{R}$ outputs that we obtain at the end of the network before our last activation function. This last activation function will be a, depending on the task at hand, sigmoid, softmax, or the identifity function for regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErcZ92U3MXbe"
      },
      "source": [
        "### 3.2 Class object neural networks and hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT_rF-1cMXbi"
      },
      "source": [
        "We've seen how to implement a neural network using PyTorch `nn.Sequential`. It is more flexible however to write our own model class. This allows us to have more control over which operations we use and allows us to define our own hyperparameters. To make a PyTorch model, we specify our class object to be a submodule of `nn.Module` and inherit its methods via `super().__init__()`. Further, we specify all necessary attributes (such as layers) in our `__init__` function (executed upon initialization) and implement a `forward` function, which is executed when the object is called after being initialized.\n",
        "\n",
        "The following code shows two examples. The first one corresponds to a neural network without hyperparameters. The second one correponds to the same network, however, where we make use of the `__init__` function in order to pass hyperparameters as arguments. We can for example specify a hyperparameter that specifies whether dropout is used or not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCWUPuY_I-nq"
      },
      "outputs": [],
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(50, 40)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "        self.layer2 = nn.Linear(40, 20)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "        self.layer3 = nn.Linear(20, 10)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "        self.layer4 = nn.Linear(10, 5)\n",
        "        # Think again: why do we not want a relu and dropout after our last layer again?\n",
        "\n",
        "    def forward(self, x):\n",
        "        # call them in separate lines:\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # or together:\n",
        "        x = self.dropout2(self.relu2(self.layer2(x)))\n",
        "        x = self.dropout3(self.relu3(self.layer3(x)))\n",
        "\n",
        "        # we could've also wrapped everything in a nn.Sequential ..\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "class HyperparameterModel(nn.Module):\n",
        "    def __init__(self, dimensions_from_input_to_output = [50, 40, 20, 10, 5], dropout = True):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        # iterate through all layers:\n",
        "        for i in range(len(dimensions_from_input_to_output) - 2):\n",
        "            layer = nn.Linear(dimensions_from_input_to_output[i], dimensions_from_input_to_output[i + 1])\n",
        "            layers.append(layer)\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout == True:\n",
        "                layers.append(nn.Dropout(0.2))\n",
        "\n",
        "\n",
        "        # the last layer separate from the loop because we don't want a ReLU and dropout after the last layer\n",
        "        layer = nn.Linear(dimensions_from_input_to_output[i+1], dimensions_from_input_to_output[i + 2])\n",
        "        layers.append(layer)\n",
        "\n",
        "        # wrap the layers in a sequential\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Havh_KMXbs"
      },
      "source": [
        "Let's test the first model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj1h-SUXMXbw"
      },
      "outputs": [],
      "source": [
        "net = BasicModel()\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGo96VzdMXb1"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 50)\n",
        "y = net(x)\n",
        "print(y)\n",
        "print(y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dot4uBfzMXb-"
      },
      "source": [
        "And now the second model, with default arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlVAqCvgMXcC"
      },
      "outputs": [],
      "source": [
        "net = HyperparameterModel()\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exSn3_hWMXcI"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 50)\n",
        "y = net(x)\n",
        "print(y)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN6ZLONoMXcP"
      },
      "source": [
        "And, finally, again the second model, albeit with specific arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFbjxV_9MXcS"
      },
      "outputs": [],
      "source": [
        "net = HyperparameterModel(dimensions_from_input_to_output = [50, 160, 80, 40, 20, 10, 5], dropout = False)\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35MBDkKZMXcW"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 50)\n",
        "y = net(x)\n",
        "print(y)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci8sWl7RMXcZ"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 3.1:</b> **Copy over any of the two networks above (preferably the one you understand best), and modify it to take as input 784 features and as output 10 nodes. We will use this network later in the PC lab. You can test your network for bugs using some randomly generated data (as seen above).**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuJm_kzwMXce"
      },
      "outputs": [],
      "source": [
        "######## YOUR CODE HERE #########\n",
        "\n",
        "\n",
        "#################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pqMG8vMXci"
      },
      "source": [
        "### 3.3 Data and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5dpoMHtMXcj"
      },
      "source": [
        "You may have noticed that the outputs of your model have a `grad_fn` attribute. This grad function will be used by PyTorch's automatic differentation engine to perform backward passes and compute gradients for every parameter with respect to the loss/cost function.\n",
        "\n",
        "Now that we have our model and know that PyTorch will automatically compute gradients for us, we can move on to train a model. To do this, we need the following:\n",
        "\n",
        "The most basic blueprint of PyTorch model training consists of\n",
        "- Get your data\n",
        "- Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "- Instantiate the model\n",
        "- Instantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "- Iterate through your training data, for every batch:\n",
        "    - reset the gradients\n",
        "    - do forward pass\n",
        "    - compute loss\n",
        "    - backward pass\n",
        "    - update parameters\n",
        "\n",
        "(Optionally):\n",
        "- After every full iteration through all training data samples (called an epoch), loop through all batches of validation data:\n",
        "    - forward pass\n",
        "    - compute loss and validation scores\n",
        "\n",
        "In this way, we can monitor how good our model is performing on left-out validation data during training, this is used in early stopping. Notably, we do not compute and reset gradients and update parameters during our validation iterations, because we do not want to fit our model on this data.\n",
        "\n",
        "Let's start with step one: get your data and wrap them in data loaders. We will first illustrate how we can convert our usual Pandas or NumPy datasets to be compatible with PyTorch with some random data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZG5Ikr8MXck"
      },
      "outputs": [],
      "source": [
        "X_train = np.random.randn(100, 784)\n",
        "y_train = np.random.randint(low = 0, high = 10, size = (100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bbTYpxMXcm"
      },
      "outputs": [],
      "source": [
        "X_train = torch.from_numpy(X_train)\n",
        "print(X_train.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlAd2-iMXcq"
      },
      "source": [
        "It is important to keep in mind data types: by default NumPy works with `float64`. However, if you instantiate a model in PyTorch, by default it will have weights in `float32`. It is therefore advised to convert your data to `float32`. In PyTorch, `float` is shorthand for `float32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF43PgTRMXcr"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.float()\n",
        "# Equivalent: X_train.to(torch.float) or X_train.to(torch.float32)\n",
        "print(X_train.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMTh2qpAMXcu"
      },
      "outputs": [],
      "source": [
        "y_train = torch.tensor(y_train)\n",
        "print(y_train.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnI0TnDQMXc1"
      },
      "source": [
        "Now that we have our X_train and y_train as tensors, we can make them PyTorch-ready by wrapping them in a PyTorch dataset, followed by wrapping them in a DataLoader. The latter gives access to batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMRu_hLsMXc4"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quJWFJxiMXc5"
      },
      "source": [
        "Now we can use our train_dataloader as such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv5E6nmXMXc6"
      },
      "outputs": [],
      "source": [
        "for batch in train_dataloader:\n",
        "    X, y = batch\n",
        "    print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb8HQt4MXc_"
      },
      "source": [
        "As you can see, we can iterate through the batches by using a for loop. Each batch consists of a list of X and y tensors. Let's isolate one batch for testing purposes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk4l7myPMXdF"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifxPbgJjMXdJ"
      },
      "source": [
        "For the remainder of the PC lab, we will work with the MNIST dataset included in `torchvision`. In case you want to run this code locally, you can install Torchvision by means of `pip install torchvision`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as-YG3-_MXdM"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "X_train = train_data.data\n",
        "y_train = train_data.targets\n",
        "\n",
        "X_test = test_data.data\n",
        "y_test = test_data.targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8hdN8G5MXdQ"
      },
      "outputs": [],
      "source": [
        "print('shapes:')\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "print('first training image tensor:')\n",
        "print(X_train[0])\n",
        "print('first five labels:')\n",
        "print(y_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA03h75wMXdV"
      },
      "source": [
        "Our data consists of images, where each sample consists of $28 \\times 28$ input features/pixels. In order to feed this data to our model, we will need to flatten these features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgNA_Y8gMXdW"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(-1, 28 * 28)\n",
        "X_test = X_test.reshape(-1, 28 * 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQryYAP6MXda"
      },
      "source": [
        "Next, since our (grayscale) pixel values range between 0 and 255, it is good practice to min-max scale the data by means of dividing by 255:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUXvQOqXMXdb"
      },
      "outputs": [],
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai8QjbZ9MXdh"
      },
      "source": [
        "Finally, for the sake of sanity, let's check the data types for training and test sets: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n9Pg1VxMXdj"
      },
      "outputs": [],
      "source": [
        "X_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3mE5c6nMXdk"
      },
      "source": [
        "Let's split up our training set in a training and validation set and finally wrap our data in a data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9DQFGa3MXdl"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "train_indices, val_indices = np.split(np.random.permutation(len(X_train)), [int(len(X_train)*0.8)])\n",
        "X_val = X_train[val_indices]\n",
        "y_val = y_train[val_indices]\n",
        "X_train = X_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, pin_memory=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4GXH_2MXdn"
      },
      "source": [
        "Let's visualize a random batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn3rRoSTMXdq"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "X_batch, y_batch = batch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(10, 8))\n",
        "cols, rows = 4, 4\n",
        "for i in range(cols * rows):\n",
        "    img, label = X_batch[i], y_batch[i]\n",
        "    figure.add_subplot(rows, cols, i+1)\n",
        "    plt.title(label.item())\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.reshape(-1, 28, 28).squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGmLYd15MXdw"
      },
      "source": [
        "In one of the previous exercises, we already implemented a model compatible with MNIST: 784 input features and 10 output nodes (one for each class). Hence, we can move on to loss functions and optimizers. For multi-class classification, we will need to use the [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss). Take a look at what kind of inputs this loss function expects. According to the documentation: `The input is expected to contain raw, unnormalized scores for each class.` Meaning that we can pass logits directly to this loss function, without the need to apply a softmax operation. \n",
        "\n",
        "When it comes to the optimizer, we can choose [vanilla gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) or the more commonly used [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam), which is itself a pimped version of stochastic gradient descent (with momentum).\n",
        "\n",
        "For model training, it is also crucial to set the appropriate learning rate.  The learning rate is a crucial hyperparameter in machine learning algorithms, particularly in the context of gradient-based optimization. It determines the size of the steps taken during the iterative process of updating model parameters to minimize the loss function. A learning rate which is too high may result in overshooting the minimum, leading to instability, while a too small learning rate can slow down convergence or cause the model to get stuck in a suboptimal solution. Finding the right balance is often a matter of experimentation and depends on the specific characteristics of the data and the model architecture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kZwMU_IMXd1"
      },
      "outputs": [],
      "source": [
        "model = \"...\" # your model from previous exercises here.\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) # SGD = stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGrEVraTMXd7"
      },
      "source": [
        "Now we're ready to perform training. We'll build up the training loop step-wise in the following codeblocks. Let's first start with passing one batch to the model, computing the loss, performing backpropagation and updating the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROoh6JGdMXd8"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "X_batch, y_batch = batch\n",
        "\n",
        "y_hat_batch = model(X_batch)\n",
        "\n",
        "loss = loss_function(y_hat_batch, y_batch) # Compute loss\n",
        "\n",
        "loss.backward()   # Calculate gradients\n",
        "optimizer.step()   # Update weights using defined optimizer\n",
        "\n",
        "print(X_batch.shape, y_batch.shape)\n",
        "print(y_hat_batch.shape)\n",
        "print(\"Outputs as logits, first two samples:\")\n",
        "print(y_hat_batch[:2])\n",
        "print('loss:', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hlu1gnRMXd_"
      },
      "source": [
        "Everytime we perform a training step, we should reset the gradients so that the gradients computed on the previous batch do not influence the next. We can do this by calling `.zero_grad()` on our optimizer. In practice, it is most safe to call this before every forward pass. A full epoch of training is obtained by the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiDrim2GMXeE"
      },
      "outputs": [],
      "source": [
        "all_losses = []\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    X_batch, y_batch = batch\n",
        "\n",
        "    y_hat_batch = model(X_batch)\n",
        "\n",
        "    loss = loss_function(y_hat_batch, y_batch) # Compute loss\n",
        "\n",
        "    loss.backward()   # Calculate gradients\n",
        "    optimizer.step()   # Update weights using defined optimizer\n",
        "\n",
        "    all_losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXPQQS4QMXeR"
      },
      "source": [
        "Plotting the loss function of every batch during one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgO5nCJJMXeZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(all_losses)), all_losses)\n",
        "smoothed_losses = np.convolve(all_losses, np.ones(50)/50, mode = \"valid\")\n",
        "plt.plot(np.arange(len(smoothed_losses)), smoothed_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD36b8QPMXee"
      },
      "source": [
        "We have evaluated the training progress during one epoch. What if we want to see the performance on the validation set during training, so that we can see if/when the model starts overfitting? It is recommended to perform a pass through the whole validation set after every training epoch. During this pass, it is important to block automatic gradient computation by means of `torch.no_grad()`. In addition, we need to tell PyTorch to put the model in evaluation mode, in order to turn off stochastic components such as dropout. After the validation epoch, we should put the model back in training mode. The above gives rise to the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw0wKUcNMXeh"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "true_labels = []\n",
        "losses = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        X_batch, y_batch = batch\n",
        "\n",
        "        y_hat_batch = model(X_batch)\n",
        "\n",
        "        loss = loss_function(y_hat_batch, y_batch)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        predictions.append(y_hat_batch)\n",
        "        true_labels.append(y_batch)\n",
        "\n",
        "model.train()\n",
        "\n",
        "predictions = torch.cat(predictions)\n",
        "true_labels = torch.cat(true_labels)\n",
        "accuracy = (true_labels == predictions.argmax(-1)).sum().item() / len(predictions)\n",
        "\n",
        "print(accuracy)\n",
        "print(np.mean(losses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f_yo3RiMXeq"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 3.2:</b> **Using the code above, put it all together to train a model for multiple epochs. After every epoch, print or save some training and validation statistics. Monitor how good your model is training. What things could you change? In particular, try the Adam optimizer or try using gradient descent with the momentum argument. How does this influence training speed? What is it doing? Try tweaking the learning rate. You should be able to obtain an accuracy of +- 96%.**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj0vgM2WMXeu"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "model =  # your model from previous exercises here.\n",
        "\n",
        "# loss function & optimizer\n",
        "\n",
        "for i in range(1, N_EPOCHS + 1):\n",
        "\n",
        "    # train loop\n",
        "\n",
        "    # eval loop\n",
        "\n",
        "    # record or print some variables that you want to keep track of during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04jmvNeBMXe0"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 3.3:</b> **Evaluate your model on the test set. What is the performance?**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J8XfWlPMXe2"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUoF-7B2MXfS"
      },
      "source": [
        "### Extra: Using GPUs\n",
        "\n",
        "Matrix multiplication run orders of magnitude faster on GPU hardware. If you have a local GPU and have installed a PyTorch version with GPU, you should be able to run this code locally. Otherwise, In Google Colab, you can request access to a GPU via `Runtime > Change runtime type > Hardware accelerator = GPU`.\n",
        "\n",
        "Briefly, the steps needed to train on GPUs consist of\n",
        "1. Putting your model on the GPU\n",
        "2. During your training loop, putting every batch on the GPU before the forward pass\n",
        "3. If you have a validation loop, doing the same there for every batch.\n",
        "4. If you have variables that you will use after training (e.g. predictions on the validation set), remember to return this back to the CPU, as GPUs have limited memory.\n",
        "\n",
        "In PyTorch, we put variables and models on the GPU by specifying their 'device' to be 'cuda' (the parallel computing platform for nvidia GPUs that PyTorch uses).\n",
        "\n",
        "The following code illustrates how to train your models on GPU hardware:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4q8iNSBMXfT"
      },
      "outputs": [],
      "source": [
        "X, y = next(iter(train_dataloader))\n",
        "\n",
        "model = model.to('cuda')\n",
        "print(X.device)\n",
        "X = X.to('cuda')\n",
        "print(X.device)\n",
        "\n",
        "y_hat = model(X)\n",
        "\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLAm5JACMXfU"
      },
      "source": [
        "We encourage you to try out training on GPUs during the next PC lab(s)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PClab_intro_nns.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
